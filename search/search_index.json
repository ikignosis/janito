{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Janito is a lightweight language model client focused on software development activities, enabling automation of programming and project tasks through natural language using LLMs.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<ul> <li>\u26a1 Automate code review, refactoring, and documentation</li> <li>\ud83d\udcac Interactive pair programming chat mode</li> <li>\ud83d\udda5\ufe0f Use via command-line with an optional web interface for complex manual edits</li> <li>\ud83d\udd0c Easily extend with custom tools and plugins</li> <li>\ud83d\udee1\ufe0f Robust configuration and quality checks</li> <li>\ud83d\udc68\u200d\ud83d\udcbb Designed for developers, by developers</li> </ul>"},{"location":"#learn-more","title":"\ud83d\udcda Learn More","text":"<ul> <li>What Is a Language Model Client? \u2014 Thin vs. Thick Clients explained</li> <li>Configuring Janito for DeepSeek \u2014 How to set up and use DeepSeek models</li> <li>Concepts &amp; Terminology: See the concepts section for more foundational explanations.</li> </ul>"},{"location":"DIV/","title":"DIV","text":"<p>Perfect! Here's your finalized framework titled:</p>"},{"location":"DIV/#div-discovery-implementation-validation","title":"DIV: Discovery, Implementation, Validation","text":""},{"location":"DIV/#a-structured-workflow-for-handling-software-development-requests-with-context-clarity-and-validation","title":"A structured workflow for handling software development requests with context, clarity, and validation.","text":""},{"location":"DIV/#1-discovery","title":"\ud83d\udfe6 1. Discovery","text":"<p>Understand the request in project context.</p> <ul> <li>Inputs: Request, codebase, documentation, issues/PRs  </li> <li>Activities: Clarify intent, cross-check knowledge, map to components  </li> <li>Outputs: Scoped problem statement, mapping to files/issues</li> </ul>"},{"location":"DIV/#2-implementation","title":"\ud83d\udfe8 2. Implementation","text":"<p>Deliver a change or provide clarity.</p> <ul> <li>Inputs: Scoped request, project artifacts  </li> <li>Activities: Code/config/doc changes OR explanation  </li> <li>Outputs: PR/commit, updated docs, or clear resolution</li> </ul>"},{"location":"DIV/#3-validation","title":"\ud83d\udfe9 3. Validation","text":"<p>Confirm correctness of both understanding and change.</p> <ul> <li>Inputs: Delivered solution or response, feedback  </li> <li>Activities: Review, test, confirm with requester  </li> <li>Outputs: Merged change, validated answer, closed task</li> </ul> <p>\ud83d\udd01 Iterative Flow: If validation reveals misalignment, loop back to Discovery or Implementation with refined understanding.</p> <p>This DIV framework offers a clean, repeatable model for handling requests \u2014 from bug fixes and enhancements to clarifications and investigations \u2014 all while staying grounded in the real state of your project.</p>"},{"location":"TOOLBAR-STYLING/","title":"Styling the Prompt Toolkit Toolbar in Janito","text":"<p>This document describes how styles are defined and applied to key elements in the command-line interface (CLI) toolbar using prompt_toolkit.</p>"},{"location":"TOOLBAR-STYLING/#how-toolbar-styling-works","title":"How Toolbar Styling Works","text":"<ul> <li>The toolbar lines (provider, model, role, key bindings, token usage, etc.) are generated in <code>janito/cli/chat_mode/toolbar.py</code>.</li> <li>The strings use special HTML-like tags (e.g., <code>&lt;role&gt;admin&lt;/role&gt;</code>, <code>&lt;key-label&gt;F1&lt;/key-label&gt;</code>) to mark segments for custom styling.</li> <li><code>prompt_toolkit</code>'s <code>HTML</code> formatted text parser interprets tags that match style names defined in a dictionary in <code>janito/cli/chat_mode/prompt_style.py</code>.</li> </ul>"},{"location":"TOOLBAR-STYLING/#defining-styles","title":"Defining Styles","text":"<p>All style rules are set in <code>janito/cli/chat_mode/prompt_style.py</code>, for example:</p> <pre><code>chat_shell_style = Style.from_dict({\n    'role': 'fg:#e87c32 bold',                # For &lt;role&gt;...&lt;/role&gt;\n    'provider': 'fg:#117fbf',                 # For &lt;provider&gt;...&lt;/provider&gt;\n    'key-label': 'bg:#ff9500 fg:#232323 bold',# For &lt;key-label&gt;...&lt;/key-label&gt;\n    ...\n})\n</code></pre> <ul> <li>The key in <code>Style.from_dict</code> must match the tag name used in toolbar line strings.</li> <li>Example: <code>&lt;role&gt;user&lt;/role&gt;</code> will be rendered using the <code>'role'</code> style.</li> </ul>"},{"location":"TOOLBAR-STYLING/#applying-styles-in-toolbar-output","title":"Applying Styles in Toolbar Output","text":"<p>When building toolbar lines, use HTML-like tags named for your style, not CSS style tags or attributes:</p> <p>Correct:</p> <pre><code>f'Press &lt;key-label&gt;F1&lt;/key-label&gt; for help | Role: &lt;role&gt;{role}&lt;/role&gt;'\n</code></pre> <p>Incorrect (won't work):</p> <pre><code>f'&lt;style class=\"key-label\"&gt;F1&lt;/style&gt; | Role: &lt;role&gt;{role}&lt;/role&gt;'\n</code></pre>"},{"location":"TOOLBAR-STYLING/#supported-tag-mapping","title":"Supported Tag Mapping","text":"Tag Used in Toolbar String Style Name in prompt_style.py Example Usage <code>&lt;role&gt;...&lt;/role&gt;</code> <code>'role'</code> <code>&lt;role&gt;user&lt;/role&gt;</code> <code>&lt;provider&gt;...&lt;/provider&gt;</code> <code>'provider'</code> <code>&lt;provider&gt;OpenAI&lt;/provider&gt;</code> <code>&lt;key-label&gt;...&lt;/key-label&gt;</code> <code>'key-label'</code> <code>&lt;key-label&gt;F1&lt;/key-label&gt;</code> <code>&lt;msg_count&gt;...&lt;/msg_count&gt;</code> <code>'msg_count'</code> <code>&lt;msg_count&gt;3&lt;/msg_count&gt;</code> ... ... ..."},{"location":"TOOLBAR-STYLING/#adding-or-changing-a-style","title":"Adding or Changing a Style","text":"<ol> <li>Define your style in <code>prompt_style.py</code>, for example:    <code>python    'custom': 'bg:#f7e01d fg:#222222'</code></li> <li>Mark up your toolbar string with <code>&lt;custom&gt;...&lt;/custom&gt;</code>.</li> <li>Result: prompt_toolkit will apply your custom style to those segments.</li> </ol>"},{"location":"TOOLBAR-STYLING/#example","title":"Example","text":"<p>If you want a new binding to stand out in blue:</p> <pre><code># In prompt_style.py:\n'blue-label': 'bg:#2629d4 fg:#ffffff bold',\n\n# In toolbar.py:\nreturn f'... &lt;blue-label&gt;F5&lt;/blue-label&gt;: Extra ...'\n</code></pre> <p>Troubleshooting: - If your style is not applied, check that the tag name in your string exactly matches a key in the style <code>from_dict</code>. - Do not use HTML <code>&lt;span&gt;</code>, <code>&lt;style&gt;</code>, or CSS classes; only tag names matching style dict keys work.</p> <p>See also: <code>prompt_toolkit.formatted_text.HTML</code> and prompt_toolkit styling documentation</p>"},{"location":"alternatives/","title":"Alternatives to Janito","text":"<p>There are many tools for AI-powered code assistance and project automation. Here are some notable alternatives, grouped by category:</p> Category Name Description Link \ud83d\udfe2 Open Source CLI aider Fast, open-source GPT coding in your terminal. aider.chat \ud83d\udfe2 Open Source CLI RA.Aid Autonomous software development agent with multi-step planning, research, and implementation. CLI-based, supports shell command execution, web research, and integration with aider. github.com/ai-christianson/RA.Aid \ud83d\udfe2 Open Source CLI OpenAI Codex CLI Lightweight, open-source coding agent that runs in your terminal. github.com/openai/codex \ud83d\udfe2 Open Source CLI Continue Open-source autopilot for software development. Integrates with VS Code and JetBrains, supports multiple models, and enables conversational coding and project-wide edits. continue.dev \ud83d\udfe9 VS Code Extension roo code Open-source, model-agnostic AI coding assistant for VS Code. Supports multi-file edits, guarded command execution, and deep project context. roocode.com \ud83d\udfe9 VS Code Extension cline Open-source, collaborative AI coding agent for VS Code. Autonomous, extensible, and supports multiple models. cline.bot \ud83d\udfe6 IDE-Integrated Cursor AI-powered code editor based on VS Code, with deep context and refactoring. cursor.com \ud83d\udfe6 IDE-Integrated Windsurf AI coding assistant and IDE (formerly Codeium) for VS Code, JetBrains, and its own editor. windsurf.com \ud83d\udfe6 IDE-Integrated GitHub Copilot AI pair programmer for VS Code, JetBrains, and more. github.com/features/copilot \ud83d\udfea Commercial Assistant Claude Code Anthropic\u2019s Claude models with code-focused features. github.com/anthropics/claude-code \ud83d\udfea Commercial Assistant Amazon CodeWhisperer AI code suggestions in IDEs. aws.amazon.com/codewhisperer \ud83d\udfea Commercial Assistant Tabnine AI code completion for multiple editors. tabnine.com \ud83c\udf10 Web-Based Chat ChatGPT OpenAI\u2019s web-based conversational AI. chat.openai.com \ud83c\udf10 Web-Based Chat Gemini Google\u2019s AI chat for code and general tasks. gemini.google.com \ud83c\udf10 Web-Based Chat Copilot Web GitHub Copilot\u2019s browser-based chat. github.com/features/copilot \ud83c\udf10 Web-Based Chat Claude Anthropic\u2019s Claude conversational AI. claude.ai <p>Each tool has its own strengths, focus, and integration style. Janito is unique in its open, prompt-driven, and tool-based approach\u2014see the rest of the docs for what sets it apart!</p>"},{"location":"deepseek-setup/","title":"Configuring Janito for DeepSeek","text":"<p>Janito supports DeepSeek as an LLM provider. This guide explains how to configure Janito to use DeepSeek models.</p>"},{"location":"deepseek-setup/#1-obtain-a-deepseek-api-key","title":"1. Obtain a DeepSeek API Key","text":"<ul> <li>Sign up or log in at DeepSeek to get your API key.</li> </ul>"},{"location":"deepseek-setup/#2-set-your-deepseek-api-key-in-janito","title":"2. Set Your DeepSeek API Key in Janito","text":"<p>You must specify both the API key and the provider name when configuring Janito for DeepSeek:</p> <pre><code>janito --set-api-key YOUR_DEEPSEEK_API_KEY -p deepseek\n</code></pre> <p>Replace <code>YOUR_DEEPSEEK_API_KEY</code> with your actual DeepSeek API key.</p>"},{"location":"deepseek-setup/#3-select-deepseek-as-the-provider","title":"3. Select DeepSeek as the Provider","text":"<p>You can set DeepSeek as your default provider:</p> <pre><code>janito --set provider=deepseek\n</code></pre> <p>Or specify it per command:</p> <pre><code>janito -p deepseek \"Your prompt here\"\n</code></pre>"},{"location":"deepseek-setup/#4-choose-a-deepseek-model","title":"4. Choose a DeepSeek Model","text":"<p>Janito supports the following DeepSeek models:</p> <ul> <li><code>deepseek-chat</code> (default)</li> <li><code>deepseek-reasoner</code></li> </ul> <p>To select a model:</p> <pre><code>janito -p deepseek -m deepseek-reasoner \"Your prompt here\"\n</code></pre>"},{"location":"deepseek-setup/#5-verify-your-configuration","title":"5. Verify Your Configuration","text":"<p>Show your current configuration:</p> <pre><code>janito --show-config\n</code></pre>"},{"location":"deepseek-setup/#6-troubleshooting","title":"6. Troubleshooting","text":"<ul> <li>Ensure your API key is correct and active.</li> <li>If you encounter issues, use <code>janito --list-providers</code> to verify DeepSeek is available.</li> <li>For more help, see the main Configuration Guide or run <code>janito --help</code>.</li> </ul> <p>For more details on supported models and features, see Supported Providers &amp; Models.</p>"},{"location":"driver-flow/","title":"OpenAI Driver Content Flow in Janito","text":"<p>This document explains the updated flow for how content and tool calls are processed in Janito, focusing on the new <code>ResponseReceived</code> event and agent logic. This supports both streaming and agent-tool interleaving for advanced use cases.</p>"},{"location":"driver-flow/#flow-overview","title":"Flow Overview","text":"<ol> <li> <p>Model Response Handling (Driver Layer)</p> <ul> <li>The entrypoint is <code>OpenAIModelDriver._process_driver_input</code> (in <code>janito/drivers/openai/driver.py</code>).</li> <li>The driver collects output from the model (including content parts, tool call suggestions, etc.), and emits a single <code>ResponseReceived</code> event (from <code>janito/driver_events.py</code>).</li> <li>This event contains all content, tool calls, normalized timestamps, and relevant metadata.</li> </ul> </li> <li> <p>Agent Decision Loop</p> <ul> <li>The agent (<code>LLMAgent</code>, in <code>janito/llm/agent.py</code>) processes the <code>ResponseReceived</code> event:<ul> <li>If the event includes tool calls, the agent invokes those tools using the <code>tools_adapter</code>, updates its conversation history with the tool calls and their results, and resubmits to the driver for a new response.</li> <li>If there are no tool calls, the agent yields the <code>ResponseReceived</code> event as output (ending the loop for that prompt).</li> </ul> </li> <li>This pattern enables fully automated tool-use loops, and naturally supports function-calling workflows (e.g., OpenAI function calling, tool-augmented LLMs).</li> </ul> </li> <li> <p>CLI Core Loop (Chat/Prompt Handler)</p> <ul> <li>In interactive (chat) mode, the CLI (<code>janito/cli/chat_mode/session.py</code>, within <code>ChatSession._chat_loop</code>) uses the <code>PromptHandler</code> to run the user's prompt. The handler now expects <code>ResponseReceived</code> events and handles terminal output accordingly.</li> </ul> </li> <li> <p>Event Reporting / Output</p> <ul> <li>The <code>RichTerminalReporter</code> (<code>janito/cli/rich_terminal_reporter.py</code>) is responsible for displaying content found in the <code>content_parts</code> field of <code>ResponseReceived</code> events.</li> <li>Only these high-level events are printed as main output, streamlining event handling logic and supporting new LLM APIs.</li> </ul> </li> </ol>"},{"location":"driver-flow/#sequence-diagram-updated","title":"Sequence Diagram (Updated)","text":"<pre><code>User prompt (in CLI)\n   \u2193\nPromptHandler.run_prompt \u2192 agent.chat() (yields final ResponseReceived)\n   \u2193\nOpenAI driver produces ResponseReceived (content+tools)\n   \u2193\nLLMAgent detects tool calls \u2192 executes via tools_adapter \u2192 extends history, repeats until no tool calls\n   \u2193\nResponseReceived with only content_parts (no tool calls)\n   \u2193\nRichTerminalReporter.on_ResponseReceived prints content\n</code></pre>"},{"location":"driver-flow/#key-classes-files","title":"Key Classes &amp; Files","text":"<ul> <li>janito/drivers/openai/driver.py: Implements the OpenAI driver and emits <code>ResponseReceived</code> events only.</li> <li>janito/driver_events.py: Defines the new <code>ResponseReceived</code> (and other) events.</li> <li>janito/llm/agent.py: Contains smart tool-handling agent event loop.</li> <li>janito/cli/prompt_core.py: Handles prompt execution and event iteration.</li> <li>janito/cli/rich_terminal_reporter.py: Handles printing content from <code>ResponseReceived</code> to the user.</li> <li>janito/cli/chat_mode/session.py: Interactive CLI chat session management.</li> </ul>"},{"location":"driver-flow/#notes","title":"Notes","text":"<ul> <li>This event-driven flow provides both streaming and agent-tool-in-the-loop logic for all drivers. It is compatible with OpenAI and other providers adopting similar response models.</li> <li>Tool/function calls from the model are now only seen in the aggregated <code>tool_calls</code> field of the <code>ResponseReceived</code> event.</li> <li>Consumers should migrate to listen for <code>ResponseReceived</code> events instead of the legacy granular events (<code>ContentPartFound</code>, etc.).</li> </ul> <p>For more information, see code comments in the affected files or reach out to the maintainers for architectural questions.</p>"},{"location":"driver-request-cancellation/","title":"Driver Request Cancellation in Janito","text":""},{"location":"driver-request-cancellation/#overview","title":"Overview","text":"<p>Driver request cancellation refers to the ability to halt or abort an in-progress request to an LLM driver (such as OpenAI, Anthropic, etc.) before it completes. This is important for responsive user interfaces, resource management, and handling user-initiated aborts (e.g., pressing Ctrl+C in the CLI).</p>"},{"location":"driver-request-cancellation/#current-handling","title":"Current Handling","text":"<p>Janito's core driver flow is event-driven and supports cooperative, programmatic cancellation of in-progress requests using a <code>threading.Event</code> (commonly named <code>cancel_event</code>). This event is passed through the agent and driver layers and can be set by the consumer to signal that the current request should be aborted as soon as possible.</p>"},{"location":"driver-request-cancellation/#how-cooperative-cancellation-works","title":"How Cooperative Cancellation Works","text":"<ul> <li>A <code>threading.Event</code> object is created and passed as <code>cancel_event</code> to the agent or driver interface (e.g., <code>agent.chat(..., cancel_event=cancel_event)</code>).</li> <li>Drivers and agents check the state of <code>cancel_event</code> at key points during request processing (before starting, after API calls, and during long-running operations).</li> <li>If <code>cancel_event.is_set()</code> returns True, the driver should abort further processing, avoid sending new requests, and clean up resources promptly.</li> <li>This allows both user-initiated (e.g., Ctrl+C) and programmatic cancellation (e.g., from a UI button or another thread).</li> </ul>"},{"location":"driver-request-cancellation/#user-initiated-cancellation","title":"User-Initiated Cancellation","text":"<ul> <li>In CLI mode, users may interrupt a request using standard terminal signals (e.g., Ctrl+C). The Python runtime will raise a <code>KeyboardInterrupt</code>, which is handled by the CLI session loop to stop further processing and clean up.</li> <li>The agent logic will set the <code>cancel_event</code> in response to user interruption, ensuring downstream drivers respond promptly.</li> </ul>"},{"location":"driver-request-cancellation/#agenttool-initiated-cancellation","title":"Agent/Tool-Initiated Cancellation","text":"<ul> <li>Agents, tools, or external consumers can programmatically set the <code>cancel_event</code> to abort an in-progress request.</li> <li>This enables responsive UIs and advanced workflows where cancellation may be triggered by logic other than user interruption.</li> </ul>"},{"location":"driver-request-cancellation/#implementation-details","title":"Implementation Details","text":"<ul> <li>The <code>cancel_event</code> is an optional field in the <code>DriverInput</code> dataclass (see <code>janito/llm/driver_input.py</code>).</li> <li>Drivers are expected to check for cancellation at the start of processing and after any blocking or long-running operation (see <code>janito/llm/driver.py</code> and driver subclasses).</li> <li>Example usage and code references are available in <code>docs/llm-drivers.md</code>.</li> </ul>"},{"location":"driver-request-cancellation/#future-directions","title":"Future Directions","text":"<ul> <li>Streaming APIs: For drivers that support streaming, partial results may be available up to the point of cancellation.</li> <li>Graceful Cleanup: Continued improvements to resource management and cleanup on cancellation are planned.</li> </ul>"},{"location":"driver-request-cancellation/#affected-flows-when-cancellation-is-performed-at-the-agent-level","title":"Affected Flows When Cancellation is Performed at the Agent Level","text":"<p>When a cancellation is triggered at the agent level (e.g., by setting the <code>cancel_event</code> or via user interruption such as Ctrl+C), the following flows are affected:</p> <ol> <li>Agent Main Loop (<code>chat</code> method): The main conversation loop passes the <code>cancel_event</code> to the driver and monitors for user interruptions. If a cancellation is detected, it stops further processing and signals downstream components.</li> <li>Event Processing (<code>_process_next_response</code>): This method waits for events from the driver. On user interruption, it creates a <code>RequestFinished</code> event with status <code>cancelled</code> and puts it in the input queue, propagating cancellation.</li> <li>Driver Input Preparation (<code>_prepare_driver_input</code>): The <code>cancel_event</code> is attached to the <code>DriverInput</code> object, ensuring it is available to the driver for cooperative cancellation.</li> <li>Driver Processing (<code>process_driver_input</code> in <code>LLMDriver</code> and subclasses):</li> <li>Before starting, the driver checks if <code>cancel_event</code> is set and aborts if so.</li> <li>After API calls (and during long-running operations in subclasses), the driver checks <code>cancel_event</code> again and aborts if set.</li> <li>If cancellation is detected, a <code>RequestFinished</code> event with status <code>cancelled</code> is emitted to the output queue.</li> <li>Tool/Function Execution (within <code>_handle_response_received</code>): If tool calls are in progress, cancellation may prevent further tool execution or message handling, depending on when the event is set.</li> </ol> <p>This cooperative cancellation mechanism ensures that all major flows\u2014agent loop, driver processing, and tool execution\u2014respond promptly to cancellation requests, providing a responsive and robust user experience.</p>"},{"location":"driver-request-cancellation/#recommendations","title":"Recommendations","text":"<ul> <li>Use the <code>cancel_event</code> mechanism for both user-initiated and programmatic cancellation.</li> <li>For UI or API integrations, expose a way to set the <code>cancel_event</code> to allow users or logic to abort requests.</li> <li>For more details, see <code>docs/llm-drivers.md</code> and the relevant code in <code>janito/llm/agent.py</code>, <code>janito/llm/driver.py</code>, and driver implementations.</li> </ul>"},{"location":"driver-request-cancellation/#references","title":"References","text":"<ul> <li>See <code>docs/llm-drivers.md</code> for architecture and example usage.</li> <li>See CLI session handling for interruption logic.</li> <li>See <code>janito/llm/driver_input.py</code>, <code>janito/llm/agent.py</code>, and driver implementations for code-level details.</li> </ul>"},{"location":"drivers/","title":"LLM Driver Architecture and Implementation Guide","text":"<p>This document describes the architecture of the LLM driver system in Janito, focusing on the <code>LLMDriver</code> base class and the requirements for implementing a new provider-specific driver. It uses the OpenAI driver as a reference example.</p>"},{"location":"drivers/#overview-the-llmdriver-base-class","title":"Overview: The <code>LLMDriver</code> Base Class","text":"<p>All LLM drivers in Janito inherit from the abstract base class <code>LLMDriver</code> (<code>janito/llm/driver.py</code>). This class provides a threaded, queue-based interface for interacting with language model APIs in a provider-agnostic way.</p>"},{"location":"drivers/#key-responsibilities","title":"Key Responsibilities","text":"<ul> <li>Threaded Operation: Each driver runs a background thread that processes requests from an input queue and emits results/events to an output queue.</li> <li>Standardized Events: Drivers emit standardized events (e.g., <code>RequestStarted</code>, <code>ResponseReceived</code>, <code>RequestFinished</code>) for downstream consumers.</li> <li>Provider Abstraction: The base class defines abstract methods for provider-specific logic, ensuring a uniform interface for all drivers.</li> </ul>"},{"location":"drivers/#required-abstract-methods","title":"Required Abstract Methods","text":"<p>To implement a new driver, you must subclass <code>LLMDriver</code> and implement the following methods:</p> <ul> <li><code>def _prepare_api_kwargs(self, config, conversation)</code></li> <li> <p>Prepare the keyword arguments for the provider API call, including model name, parameters, and tool schemas if needed.</p> </li> <li> <p><code>def _call_api(self, driver_input: DriverInput)</code></p> </li> <li> <p>Execute the provider API call using the prepared arguments. Should handle cancellation and error reporting.</p> </li> <li> <p><code>def _convert_completion_message_to_parts(self, message)</code></p> </li> <li> <p>Convert the provider's response message into a list of standardized <code>MessagePart</code> objects (e.g., text, tool calls).</p> </li> <li> <p><code>def convert_history_to_api_messages(self, conversation_history)</code></p> </li> <li> <p>Convert the internal conversation history to the format required by the provider's API (e.g., a list of dicts for OpenAI).</p> </li> <li> <p><code>def _get_message_from_result(self, result)</code></p> </li> <li>Extract the relevant message object from the provider's API result for further processing.</li> </ul>"},{"location":"drivers/#threading-and-queues","title":"Threading and Queues","text":"<ul> <li>Each driver instance has its own <code>input_queue</code> and <code>output_queue</code>.</li> <li>Use the <code>start()</code> method to launch the driver's background thread.</li> <li>Submit requests by putting <code>DriverInput</code> objects into <code>input_queue</code>.</li> <li>Listen for events/results by reading from <code>output_queue</code>.</li> </ul>"},{"location":"drivers/#implementing-a-new-driver-checklist","title":"Implementing a New Driver: Checklist","text":"<ol> <li>Subclass <code>LLMDriver</code>.</li> <li>Implement all required abstract methods listed above.</li> <li>Handle provider-specific configuration (e.g., API keys, endpoints) in your constructor or via config objects.</li> <li>Emit standardized events using the provided event classes (<code>RequestStarted</code>, <code>ResponseReceived</code>, <code>RequestFinished</code>).</li> <li>Support cancellation by checking the <code>cancel_event</code> in <code>DriverInput</code> before and after API calls.</li> <li>Convert conversation history to the provider's required format.</li> <li>Convert provider responses to standardized message parts for downstream processing.</li> </ol>"},{"location":"drivers/#example-openai-driver","title":"Example: OpenAI Driver","text":"<p>See <code>janito/drivers/openai/driver.py</code> for a complete example. Highlights: - Implements all required methods for the OpenAI API. - Handles tool/function call schemas if tools are present. - Converts conversation history to OpenAI's message format. - Extracts usage and other metadata from the API response. - Handles cancellation and error reporting robustly.</p>"},{"location":"drivers/#references","title":"References","text":"<ul> <li>Base class: <code>janito/llm/driver.py</code></li> <li>OpenAI driver: <code>janito/drivers/openai/driver.py</code></li> <li>Driver events: <code>janito/driver_events.py</code></li> </ul>"},{"location":"event-bus/","title":"Event Bus System Documentation","text":""},{"location":"event-bus/#overview","title":"Overview","text":"<p>The event bus is a central mechanism for publish/subscribe (pub/sub) communication in the system. It enables decoupled components to communicate by publishing and subscribing to events of various types.</p> <ul> <li>Location: <code>janito/event_bus/bus.py</code></li> <li>Singleton Instance: <code>event_bus</code></li> </ul>"},{"location":"event-bus/#event-architecture","title":"Event Architecture","text":"<ul> <li>Events are Python dataclasses (see <code>janito/event_types.py</code>) that represent occurrences or state changes in the system.</li> <li>Event Types are defined as subclasses of the base <code>Event</code> class (now located in <code>janito/event_bus/event.py</code>).</li> <li>Subscribers are functions or objects that listen for specific event types.</li> </ul>"},{"location":"event-bus/#defining-events","title":"Defining Events","text":"<ol> <li>Base Event Class:</li> <li>Located at <code>janito/event_bus/event.py</code>:      <code>python      from dataclasses import dataclass      from typing import ClassVar      @dataclass      class Event:          category: ClassVar[str] = \"generic\"</code></li> <li>Custom Event Types:</li> <li>Define new events by subclassing <code>Event</code> or its descendants in <code>janito/event_types.py</code>.</li> <li>Example:      <code>python      @dataclass      class RequestStarted(DriverEvent): class RequestFinished(DriverEvent):          payload: Any          # ...</code></li> </ol>"},{"location":"event-bus/#subscribing-to-events","title":"Subscribing to Events","text":"<p>To listen for events, subscribe a callback to an event type:</p> <pre><code>from janito.event_bus.bus import event_bus\nfrom janito.driver_events import RequestStarted, RequestFinished\n\ndef on_request_started(event):\n    print(f\"Request started: {event}\")\n\nevent_bus.subscribe(RequestStarted, on_request_started)\nevent_bus.subscribe(RequestFinished, on_request_finished)\n</code></pre>"},{"location":"event-bus/#unsubscribing-from-events","title":"Unsubscribing from Events","text":"<p>To stop listening:</p> <pre><code>event_bus.unsubscribe(RequestStarted, on_request_started)\nevent_bus.unsubscribe(RequestFinished, on_request_finished)\n</code></pre>"},{"location":"event-bus/#publishing-events","title":"Publishing Events","text":"<p>To notify subscribers of an event:</p> <pre><code>from janito.driver_events import RequestStarted, RequestFinished\nfrom janito.event_bus.bus import event_bus\n\nmy_event = RequestStarted(driver_name=\"driver1\", request_id=\"abc123\", payload={...})\nevent_bus.publish(my_event)\n</code></pre>"},{"location":"event-bus/#automatic-timestamping","title":"Automatic Timestamping","text":"<ul> <li>Every event published will have a <code>timestamp</code> attribute (UNIX epoch seconds) automatically set by the event bus.</li> <li>This is injected at publish time and is available to all subscribers:   <code>python   def on_request_started(event):       print(event.timestamp)  # Set automatically by event bus</code></li> </ul>"},{"location":"event-bus/#example","title":"Example","text":"<pre><code>from janito.driver_events import RequestStarted, RequestFinished\nfrom janito.event_bus.bus import event_bus\n\ndef log_event(event):\n    print(f\"[{event.timestamp}] Event: {event}\")\n\nevent_bus.subscribe(RequestStarted, log_event)\n\n# Later in code...\nevent = RequestStarted(driver_name=\"driver1\", request_id=\"abc123\", payload={\"foo\": \"bar\"})\nevent_bus.publish(event)\n</code></pre>"},{"location":"event-bus/#best-practices","title":"Best Practices","text":"<ul> <li>Do not manually add a <code>timestamp</code> field to event dataclasses; it is managed by the event bus.</li> <li>Use specific event types for clarity and maintainability.</li> <li>Unsubscribe handlers when they are no longer needed to avoid memory leaks.</li> </ul> <p>For more details, see the source code in <code>janito/event_bus/bus.py</code> and <code>janito/event_types.py</code>.</p>"},{"location":"llm-drivers-required-config/","title":"LLM Driver Required Config Pattern","text":"<p>Some LLM drivers (when implemented) may require additional configuration fields (beyond API key or model name) to operate correctly. The <code>required_config</code> class attribute is intended to enable each driver to declare these requirements explicitly, and for providers to validate config early. As of this writing, no LLM driver implementation is present in this directory; this document describes the intended pattern.</p>"},{"location":"llm-drivers-required-config/#how-to-use","title":"How to Use","text":"<ol> <li> <p>Declare requirements in your driver:</p> <p><code>python class AzureOpenAIModelDriver(OpenAIModelDriver):     required_config = {\"azure_endpoint\"}  # The config dict must contain this key.</code></p> </li> <li> <p>Validation on driver instantiation:</p> <p>Instantiation via <code>LLMProvider.get_driver_for_model</code> will check that all required fields are present in the passed config dict, and raise a <code>ValueError</code> if any are missing.</p> </li> <li> <p>Backwards compatible:</p> </li> <li>If <code>required_config</code> is not present, no validation is performed.</li> <li>Providers and code using drivers without required_config are unchanged.</li> </ol>"},{"location":"llm-drivers-required-config/#example-azure-openai","title":"Example: Azure OpenAI","text":"<p>If a model spec for Azure OpenAI uses <code>AzureOpenAIModelDriver</code>, the following config is required:</p> <pre><code>config = {\n    \"azure_endpoint\": \"https://example.openai.azure.com/\"\n}\n</code></pre> <p>Attempting to create the driver without this field will result in:</p> <pre><code>ValueError: Missing required config for AzureOpenAIModelDriver: azure_endpoint\n</code></pre>"},{"location":"llm-drivers-required-config/#extending-to-other-drivers","title":"Extending to Other Drivers","text":"<p>Other drivers may declare their own required fields (e.g., project_id, base_url) by providing a <code>required_config</code> class attribute as a set or list of key names.</p> <p>This pattern promotes robust, explicitly validated configuration for LLM drivers.</p>"},{"location":"llm-drivers/","title":"LLM Drivers Architecture","text":""},{"location":"llm-drivers/#overview","title":"Overview","text":"<p>The driver layer described below is intended to provide a unified, event-driven interface for interacting with various Large Language Model (LLM) providers (such as OpenAI, Google Gemini, MistralAI, etc.). However, as of this writing, the actual driver code (including the LLMDriver base class and its subclasses) is not present in this directory. The following describes the intended architecture and requirements, but no implementation is currently available here.</p>"},{"location":"llm-drivers/#key-concepts","title":"Key Concepts","text":""},{"location":"llm-drivers/#streaming-event-driven-interface","title":"Streaming, Event-Driven Interface","text":"<ul> <li>All drivers now use a threaded, queue-based input/output mechanism. The agent sends DriverInput objects to the input queue and reads aggregate DriverEvent objects (notably <code>ResponseReceived</code>) from the output queue.</li> <li>Drivers emit standardized events (e.g., <code>ResponseReceived</code>, <code>GenerationStarted</code>, <code>RequestFinished</code>, etc.) as the generation progresses. <code>RequestFinished</code> covers both success, error, and cancellation cases.</li> <li>The new <code>ResponseReceived</code> event contains all content, tool calls, and metadata for that turn, so consumers and agents can react more intelligently (especially for automatic tool invocation patterns).</li> </ul>"},{"location":"llm-drivers/#threading-and-cancellation","title":"Threading and Cancellation","text":"<ul> <li>The generation process runs in a background thread, ensuring that the main application/UI remains responsive.</li> <li>Cooperative cancellation is supported via a <code>threading.Event</code> passed to <code>stream_generate()</code>. Consumers can set this event to abort generation early.</li> <li>Once cancellation is received (i.e., the event is set), drivers will not execute any new tools or send any new requests to the LLM provider. Ongoing operations will be stopped as soon as possible, ensuring prompt and safe cancellation.</li> </ul>"},{"location":"llm-drivers/#consistency-and-extensibility","title":"Consistency and Extensibility","text":"<ul> <li>All drivers inherit from the <code>LLMDriver</code> abstract base class and follow the same event and threading conventions.</li> <li>Each driver handles provider-specific API calls, tool/function execution, and event emission internally, but always exposes the same external interface.</li> </ul>"},{"location":"llm-drivers/#example-usage","title":"Example Usage","text":"<pre><code>import threading\nfrom janito.driver_events import ResponseReceived, RequestFinished\n\ncancel_event = threading.Event()\nfor event in agent.chat(\n    prompt=\"Tell me a joke.\",\n    system_prompt=\"You are a witty assistant.\",\n    cancel_event=cancel_event\n):\n    if isinstance(event, ResponseReceived):\n        for part in event.content_parts:\n            print(part, end=\"\", flush=True)\n    elif isinstance(event, RequestFinished) and getattr(event, 'status', None) == 'error':\n        print(f\"\\n[Error: {event.error}]\")\n</code></pre>"},{"location":"llm-drivers/#supported-events","title":"Supported Events","text":"<ul> <li><code>ResponseReceived</code>: Aggregate response event containing all content parts, all tool calls, and associated metadata for the turn. The agent now listens for this event by default.</li> <li><code>GenerationStarted</code>: Generation process has begun.</li> <li><code>RequestStarted</code>, <code>RequestFinished</code>: API request lifecycle events. <code>RequestFinished</code> includes a <code>status</code> field which may be 'success', 'error', or 'cancelled'.</li> <li>(Legacy granular events such as <code>ContentPartFound</code> are no longer emitted by compliant drivers.)</li> <li>(Provider-specific events may also be emitted.)</li> </ul>"},{"location":"llm-drivers/#adding-a-new-driver","title":"Adding a New Driver","text":"<p>To add support for a new LLM provider:</p> <ol> <li>Subclass <code>LLMDriver</code>.</li> <li>Implement the <code>_process_driver_input()</code> method, which consumes a DriverInput object, performs LLM generation, and emits DriverEvent objects to the output queue.</li> <li>Emit standardized events as output is generated.</li> </ol>"},{"location":"llm-drivers/#provider-specific-notes","title":"Provider-Specific Notes","text":""},{"location":"llm-drivers/#google-gemini-genai-driver","title":"Google Gemini (genai) Driver","text":"<p>The Google Gemini driver (and all other modern drivers) now emits a single <code>ResponseReceived</code> event per turn, which includes both all content parts and all tool/function calls as parsed from the Gemini API response. Downstream consumers and the agent itself inspect the order and content of these lists to reproduce the true conversational order and context, enabling seamless advanced tool execution. No more per-part events if the driver is up-to-date.</p>"},{"location":"llm-drivers/#design-philosophy","title":"Design Philosophy","text":"<ul> <li>Responsiveness: All generation is non-blocking and can be cancelled at any time.</li> <li>Observability: Consumers can react to fine-grained events for real-time UIs, logging, or chaining.</li> <li>Simplicity: A single, modern interface for all drivers.</li> </ul>"},{"location":"supported-providers-models/","title":"Supported Providers &amp; Models","text":"<p>\ud83d\ude80 Janito is optimized and tested for the default model: <code>gpt-4.1</code>. \ud83e\uddea Testing and feedback for other models is welcome!</p>"},{"location":"supported-providers-models/#model-types","title":"\ud83e\udd16 Model Types","text":"<p>Janito is compatible with most OpenAI-compatible chat models, including but not limited to:</p> <ul> <li><code>gpt-4.1</code> (default)</li> <li>Azure-hosted OpenAI models (with correct deployment name)</li> </ul>"},{"location":"supported-providers-models/#how-to-select-a-model","title":"\ud83d\udee0\ufe0f How to Select a Model","text":"<ul> <li>Use the <code>--model</code> CLI option to specify the model for a session:   <code>janito \"Prompt here\" --model gpt-4.1</code></li> <li>Configure your API key and endpoint in the configuration file or via CLI options.</li> </ul>"},{"location":"supported-providers-models/#i-notes","title":"\u2139\ufe0f Notes","text":"<ul> <li>Some advanced features (like tool calling) require models that support OpenAI function calling.</li> <li>Model availability and pricing depend on your provider and API key.</li> <li>For the latest list of supported models, see your provider\u2019s documentation or the OpenAI models page.</li> </ul>"},{"location":"terms/","title":"Terms","text":""},{"location":"terms/#comprehensive-api-interaction-terminology","title":"\u2705 Comprehensive API Interaction Terminology","text":"Term Description Model Provider The company or project offering API access to a language model. Examples: OpenAI, Azure OpenAI Service, Anthropic, Google (Gemini), Mistral, Meta (LLaMA), Cohere, Aleph Alpha, xAI (Grok), AWS Bedrock (multi-provider). Model The specific language model you interact with. Examples: <code>gpt-4-turbo</code>, <code>claude-3-opus</code>, <code>gemini-pro</code>. System Prompt A special instruction that sets the model\u2019s behavior, role, or context before processing user prompts. Prompt The input message or instruction sent by the user to the model. Completion / Response The model's generated reply to the user prompt. Token The smallest unit of text processed for billing and context limits. Includes input and output tokens. Context Window The maximum number of tokens the model can handle in a single request, combining prompt and response. Inference The process of generating a model response from the provided input. Temperature Controls output randomness. Lower = more deterministic, higher = more creative. Top-k / Top-p Sampling Controls randomness by limiting or filtering possible next tokens based on rank or probability. Latency The time taken to return a response after sending a request. Rate Limit The maximum number of allowed requests per unit of time, enforced by the provider. API Key A secret credential used to authenticate and authorize API access. Quota / Billing Usage tracking or charges, usually based on token counts or request volume."},{"location":"terms/#interaction-types","title":"\u2705 Interaction Types","text":"Term Description Single-Turn Interaction One user prompt and one final model response. May include automatic tool use loops before producing the response. Multi-Turn Conversation A sequence of user and model messages, maintaining conversational context across turns."},{"location":"terms/#tool-calling-and-automation-terms","title":"\u2705 Tool-Calling and Automation Terms","text":"Term Description Tool Call / Function Call A structured API call generated by the model to invoke external functionality (e.g., weather lookup, database query). Tool Call Response The data or result returned by the external tool in response to the model\u2019s tool call. Tool Use Auto Loop An automatic process where the model issues one or more tool calls, receives results, and continues reasoning until it produces a final response\u2014all within a single user interaction."},{"location":"tools-natural-results/","title":"Natural Results: Human-Friendly Output from Janito Tools","text":""},{"location":"tools-natural-results/#why-janito-tools-use-unstructured-line-based-output","title":"Why Janito Tools Use Unstructured, Line-Based Output","text":"<p>Janito's tools are designed to provide results in a natural, unstructured, line-based format\u2014the same style commonly found in code examples, tutorials, and instructional materials. This approach is intentional and is based on several key considerations:</p>"},{"location":"tools-natural-results/#1-familiarity-and-clarity","title":"1. Familiarity and Clarity","text":"<ul> <li>Most developers are accustomed to reading and understanding code in its natural, unannotated form. Code examples, documentation, and learning resources rarely use diff formats; instead, they present the code as it should appear after edits.</li> <li>By outputting results in this familiar format, Janito ensures that users can quickly understand and apply the changes without needing to mentally parse diff markers or context lines.</li> </ul>"},{"location":"tools-natural-results/#2-avoiding-out-of-context-patterns","title":"2. Avoiding Out-of-Context Patterns","text":"<ul> <li>Diff-based formats (such as unified diffs with <code>+</code>, <code>-</code>, or <code>@@</code> markers) are excellent for code review and version control, but they introduce artificial patterns and symbols that are not part of the actual code.</li> <li>When these patterns are present in the editing or code generation flow, they can inadvertently influence the language model or the user's perception, potentially leading to lower-quality code or confusion.</li> <li>Janito optimizes for clean, context-free code generation, reducing the risk of such artifacts affecting the output.</li> </ul>"},{"location":"tools-natural-results/#3-optimized-for-human-editing","title":"3. Optimized for Human Editing","text":"<ul> <li>The primary goal of Janito's output is to facilitate smooth, human-friendly editing. Users can copy, paste, and apply changes directly, just as they would with code snippets from trusted documentation.</li> <li>This approach streamlines the workflow for developers who want to quickly update their codebase without extra processing or translation steps.</li> </ul>"},{"location":"tools-natural-results/#4-review-remains-easy-with-standard-tools","title":"4. Review Remains Easy with Standard Tools","text":"<ul> <li>While Janito does not output diffs directly, users can still perform thorough code reviews using standard version control tools (like <code>git diff</code>) after applying the changes.</li> <li>This separation of concerns ensures that code generation and review are both optimized for their respective contexts: natural output for editing, and diff-based tools for review.</li> </ul>"},{"location":"tools-natural-results/#summary","title":"Summary","text":"<p>Janito's natural, line-based output format is designed to:</p> <ul> <li>Maximize clarity and usability for developers.</li> <li>Avoid introducing out-of-context patterns that could degrade code quality.</li> <li>Support efficient, human-friendly editing flows.</li> <li>Allow for robust reviews using existing diff tools after changes are applied.</li> </ul> <p>This philosophy ensures that Janito remains a seamless, developer-centric assistant\u2014helping you write, edit, and improve code in the most natural way possible.</p>"},{"location":"tools-precision/","title":"Precision in Context Construction: Outlines, Search, and Token Optimization","text":"<p>Large Language Models (LLMs) like those used in Janito are powerful, but their effectiveness depends heavily on the quality and relevance of the context provided to them. Precision in context construction is crucial for:</p> <ul> <li>Improving the model\u2019s attention and accuracy.</li> <li>Reducing irrelevant information (noise).</li> <li>Optimizing the use of available tokens (which are limited per request).</li> </ul>"},{"location":"tools-precision/#why-precision-matters","title":"Why Precision Matters","text":"<p>LLMs have a fixed token limit for each prompt. Supplying too much irrelevant or excessive context can:</p> <ul> <li>Waste valuable tokens.</li> <li>Dilute the model\u2019s focus, leading to less accurate or less relevant responses.</li> </ul> <p>By contrast, providing only the most relevant code, documentation, or data enables the LLM to:</p> <ul> <li>Focus its attention on what matters for the current task.</li> <li>Produce more accurate, actionable, and context-aware outputs.</li> </ul>"},{"location":"tools-precision/#how-janito-achieves-precision","title":"How Janito Achieves Precision","text":"<p>Janito uses a combination of outline and search utilities to extract only the most relevant portions of code or documentation:</p>"},{"location":"tools-precision/#1-outline-utilities","title":"1. Outline Utilities","text":"<ul> <li>Purpose: Quickly analyze the structure of files (e.g., Python modules, Markdown docs) to identify classes, functions, methods, headers, and sections.</li> <li>How it works:</li> <li>The outline tool parses the file and builds a map of its structure.</li> <li>This enables Janito to select specific ranges (e.g., a single function, class, or section) rather than the entire file.</li> <li>Benefits:</li> <li>Enables targeted extraction.</li> <li>Reduces the amount of irrelevant context.</li> </ul>"},{"location":"tools-precision/#2-search-utilities","title":"2. Search Utilities","text":"<ul> <li>Purpose: Find precise locations of keywords, function names, class names, or documentation headers within files or across the project.</li> <li>How it works:</li> <li>The search tool can use substring or regex matching to locate relevant lines or blocks.</li> <li>Results are mapped to file ranges or outline nodes, allowing for precise extraction.</li> <li>Benefits:</li> <li>Supports both broad and fine-grained queries.</li> <li>Can be combined with outline data for even more accurate targeting.</li> </ul>"},{"location":"tools-precision/#building-tailored-contexts","title":"Building Tailored Contexts","text":"<p>When Janito receives a request (e.g., \"Refactor function X\" or \"Summarize section Y\"), it:</p> <ol> <li>Uses outline and search tools to locate the exact code or documentation range relevant to the task.</li> <li>Extracts only that range (plus minimal necessary context, such as imports or docstrings).</li> <li>Constructs the LLM prompt using just the tailored content, not the entire file or project.</li> </ol>"},{"location":"tools-precision/#benefits-for-llm-attention-and-token-efficiency","title":"Benefits for LLM Attention and Token Efficiency","text":"<ul> <li>Improved Attention: The LLM can focus on the most relevant code or documentation, leading to better understanding and more accurate results.</li> <li>Token Optimization: By sending only what\u2019s needed, Janito avoids hitting token limits and can handle larger projects or more complex tasks within the same constraints.</li> <li>Faster, More Relevant Responses: Less noise means the model can reason more effectively and respond more quickly.</li> </ul>"},{"location":"tools-precision/#summary","title":"Summary","text":"<p>Janito\u2019s precision-driven approach\u2014using outline and search utilities to extract and assemble only the most relevant context\u2014maximizes the effectiveness of LLMs. This ensures:</p> <ul> <li>Higher quality answers.</li> <li>Better use of computational resources.</li> <li>A more scalable and robust developer experience.</li> </ul>"},{"location":"about/costs/","title":"\ud83d\udcb8 Costs &amp; Value Transparency","text":""},{"location":"about/costs/#how-janito-handles-costs","title":"\ud83d\udca1 How Janito Handles Costs","text":"<ul> <li>\ud83c\udd93 No Extra Fees: Janito is open source and does not charge any additional fees for usage.</li> <li>\ud83d\udd11 Bring Your Own API Key: By default, Janito uses OpenAI, but you can also connect your own Azure or compatible API key. You pay only for what you use, directly to the provider.</li> <li>\ud83d\udc40 Full Visibility: You can monitor your API usage and costs through your provider\u2019s dashboard, with no hidden markups.</li> </ul>"},{"location":"about/costs/#comparison-subscription-models-in-other-tools","title":"\ud83d\udd04 Comparison: Subscription Models in Other Tools","text":"<ul> <li>\u2705 Costs Under Control: Subscription models can help users predict their monthly expenses, providing cost certainty regardless of usage spikes.</li> <li>\ud83d\udcb3 Flat Monthly Fees: Many AI coding assistants or IDE plugins charge a monthly subscription, regardless of how much you use them.</li> <li>\ud83d\udd73\ufe0f Opaque Value: These tools manage the context, prompts, and API usage behind the scenes. You don\u2019t know how much of your subscription is spent on actual model calls versus overhead or unused features.</li> <li>\ud83d\udeab Limited Control: You can\u2019t tune the context window, prompt, or tool usage to optimize for cost or value.</li> </ul>"},{"location":"about/costs/#context-optimization-and-token-efficiency","title":"\ud83e\udde0 Context Optimization and Token Efficiency","text":"<ul> <li>\ud83e\udde9 Smart Context Selection: Janito\u2019s tools are designed to select only the most relevant files, code snippets, or configuration details for each request, based on your prompt and intent.</li> <li>\ud83c\udfaf Aligned with Your Goals: Instead of sending your entire project or irrelevant data, Janito tailors the context to what you actually need\u2014whether that\u2019s UI, backend, or documentation.</li> <li>\ud83d\udcb0 Token Usage Efficiency: By minimizing unnecessary context, Janito helps you get more value from each API call, reducing token usage and cost while maximizing the quality of responses.</li> </ul>"},{"location":"about/costs/#janitos-advantage-cost-transparency","title":"\ud83d\udd0d Janito\u2019s Advantage: Cost Transparency","text":"<ul> <li>\ud83d\udcb8 Pay for What You Use: Every API call is under your control. You decide when and how to use the model, and can optimize prompts or tool usage for efficiency.</li> <li>\ud83d\udeab No Hidden Usage: There\u2019s no \u201cblack box\u201d between you and the model\u2014no risk of surprise overages or wasted spend.</li> <li>\ud83c\udfaf Direct Value: You see exactly how your usage translates to results, and can adjust your workflow to maximize value for your spend.</li> </ul>"},{"location":"about/costs/#summary","title":"\ud83c\udfc1 Summary","text":"<p>With Janito, you\u2019re in control: no subscriptions, no hidden fees, and full transparency into how your costs deliver value.</p>"},{"location":"about/vs-webchats/","title":"\ud83d\udcac Janito vs. Web-Based AI Chats","text":"<p>While web-based AI chats (like ChatGPT, Gemini, or Copilot web) are popular for code help, Janito offers significant advantages for project analysis and automation:</p>"},{"location":"about/vs-webchats/#overhead-of-manual-copypaste","title":"\u23f3 Overhead of Manual Copy/Paste","text":"<ul> <li>\ud83d\udd52 Time-Consuming: Copying large files, code snippets, or project structures into a web chat is slow and repetitive.</li> <li>\ud83d\udd0d Context Loss: Web chats lack awareness of your full project structure, dependencies, and configuration.</li> <li>\u26a0\ufe0f Error-Prone: Manual copy/paste increases the risk of missing files, truncating code, or introducing formatting errors.</li> <li>\ud83d\udeab No Automation: Each session is isolated; you can\u2019t script or automate workflows across your project.</li> </ul>"},{"location":"about/vs-webchats/#risks-to-accuracy-and-privacy","title":"\ud83d\udd12 Risks to Accuracy and Privacy","text":"<ul> <li>\u274c Incomplete Analysis: Web chats only see what you paste, missing important context or related files.</li> <li>\ud83e\uddd1\u200d\ud83d\udcbb Human Error: It\u2019s easy to accidentally omit, duplicate, or mislabel code when copying manually.</li> <li>\ud83d\udd75\ufe0f\u200d\u2642\ufe0f Privacy Concerns: Pasting proprietary code into a third-party web service may violate company policy or expose sensitive information.</li> </ul>"},{"location":"about/vs-webchats/#janitos-advantages","title":"\ud83c\udfc6 Janito\u2019s Advantages","text":"<ul> <li>\ud83d\udcc2 Direct Project Access: Janito can analyze your entire codebase, follow imports, and reference configuration automatically.</li> <li>\ud83d\udd01 Repeatable &amp; Scriptable: Automate reviews, refactoring, or documentation with CLI commands and scripts.</li> <li>\ud83e\udde0 Rich Context: Tools and prompts can be tailored to your workflow, ensuring the AI sees the right context every time.</li> <li>\u2702\ufe0f No Copy/Paste Needed: Save time and reduce mistakes by working directly with your files.</li> </ul>"},{"location":"about/vs-webchats/#summary","title":"\ud83c\udfc1 Summary","text":"<p>Janito eliminates the friction, risk, and limitations of manual copy/paste into web chats\u2014delivering faster, more accurate, and more secure project assistance.</p>"},{"location":"about/why/","title":"\u2753 Why Janito? The Power of Prompt and Tool Flexibility","text":"<p>Janito is designed to give you maximum control over how AI assists you with your codebase. Unlike tools tightly coupled with a specific IDE, Janito\u2019s flexible system prompt and tool configuration offer unique advantages:</p>"},{"location":"about/why/#benefits-of-adjustable-system-prompts","title":"\ud83c\udfaf Benefits of Adjustable System Prompts","text":"<ul> <li>\ud83d\udcdd Tailor the Assistant\u2019s Perspective: By editing the system prompt, you can instruct the AI to focus on what matters most\u2014UI/UX, backend logic, architecture, or even documentation style.</li> <li>\ud83d\udd04 Adapt to Different Tasks: Switch between reviewing code for bugs, generating documentation, or brainstorming features simply by changing the prompt.</li> <li>\ud83d\udc40 Guide the Model\u2019s Focus: A prompt like \u201cYou are a UI/UX expert\u201d will make the model prioritize interface concerns, while \u201cFocus on code structure and algorithms\u201d will drive it to analyze raw code logic.</li> </ul>"},{"location":"about/why/#flexible-tools-vs-ide-coupled-plugins","title":"\ud83e\udde9 Flexible Tools vs. IDE-Coupled Plugins","text":"<ul> <li>\ud83d\udcbb Not Locked to an Editor: Janito works in the terminal, scripts, or web\u2014no need for a heavyweight IDE or plugin ecosystem.</li> <li>\ud83e\uddf0 Composable and Extensible: Tools can be enabled, disabled, or customized for each session, letting you experiment or automate workflows.</li> <li>\ud83d\udd52 Session-Scoped Adjustments: Temporary overrides (via CLI options or prompt tweaks) let you try new approaches without changing your global setup.</li> </ul>"},{"location":"about/why/#example-prompt-driven-focus","title":"\ud83e\uddd1\u200d\ud83d\udcbb Example: Prompt-Driven Focus","text":"<ul> <li> <p>A prompt like:</p> <p>\u201cYou are a code reviewer. Focus on accessibility and UI clarity.\u201d   will make the model highlight interface and user experience issues.</p> </li> <li> <p>Change the prompt to:</p> <p>\u201cYou are a backend architect. Ignore UI, focus on data flow and performance.\u201d   and the model will shift its analysis accordingly.</p> </li> </ul>"},{"location":"about/why/#summary","title":"\ud83c\udfc1 Summary","text":"<p>Janito\u2019s decoupled, prompt-driven approach empowers you to get the AI help you need, where and how you want it\u2014without being boxed in by IDE limitations or rigid workflows.</p>"},{"location":"code_intelligence/agentic-frameworks-comparison/","title":"Why Janito Uses a Built-in Agentic Framework for Code","text":""},{"location":"code_intelligence/agentic-frameworks-comparison/#overview","title":"Overview","text":"<p>Janito is designed with a built-in agentic framework tailored specifically for code generation, analysis, and editing. This approach is fundamentally different from most general-purpose agentic frameworks, which are typically optimized for structured data extraction and workflow automation.</p>"},{"location":"code_intelligence/agentic-frameworks-comparison/#general-purpose-agentic-frameworks","title":"General-purpose Agentic Frameworks","text":"<ul> <li>Primary Focus:</li> <li>Extracting structured data from unstructured text (e.g., forms, tables, summaries).</li> <li>Automating business processes, information retrieval, or conversational flows.</li> <li>Strengths:</li> <li>Well-suited for tasks where the output is a set of fields, entities, or facts.</li> <li>Often rely on templates, schemas, or predefined extraction rules.</li> <li>Limitations for Code:</li> <li>Lack deep understanding of code semantics, dependencies, and context.</li> <li>Not designed for precise, context-aware code editing or refactoring.</li> <li>Struggle with the fragility and interconnectedness of code (see Challenges of Code Generation and Editing for LLMs).</li> </ul>"},{"location":"code_intelligence/agentic-frameworks-comparison/#janitos-built-in-agentic-framework","title":"Janito\u2019s Built-in Agentic Framework","text":"<ul> <li>Primary Focus:</li> <li>Code search, analysis, editing, and refactoring.</li> <li>Maintaining traceability and explicit error handling.</li> <li>Supporting developer workflows and codebase evolution.</li> <li>Key Features:</li> <li>Step-by-step code search and inference, with user-visible progress (see Code Generation Observability).</li> <li>Validation and testing of code changes.</li> <li>Awareness of code structure, dependencies, and side effects.</li> <li>Designed for iterative, collaborative development.</li> </ul>"},{"location":"code_intelligence/agentic-frameworks-comparison/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Reliability: Code changes require precision and context-awareness\u2014mistakes can break systems.</li> <li>Traceability: Developers need to understand and audit every change, not just the final output.</li> <li>Developer Experience: Janito\u2019s workflow is optimized for real-world coding, not just data extraction.</li> </ul>"},{"location":"code_intelligence/agentic-frameworks-comparison/#conclusion","title":"Conclusion","text":"<p>Janito\u2019s tailored agentic framework is purpose-built for the unique challenges of code generation and editing. It provides the transparency, precision, and control that developers need\u2014going far beyond what general-purpose agentic frameworks can offer for code-centric tasks.</p> <p>generated by janito.dev</p>"},{"location":"code_intelligence/code-generation-challenges/","title":"Challenges of Code Generation and Editing for LLMs","text":"<p>Code Intelligence provides human-in-control intelligence for code understanding, editing, and reliability.</p>"},{"location":"code_intelligence/code-generation-challenges/#what-is-code","title":"What is \"Code\"?","text":"<p>\"Code\" refers to formal instructions written in programming languages. Unlike natural language, code is designed for unambiguous interpretation by machines. Code has strict syntax, grammar, and semantics, and even small deviations can cause errors or unintended behavior.</p>"},{"location":"code_intelligence/code-generation-challenges/#code-semantics-vs-natural-language-semantics","title":"Code Semantics vs. Natural Language Semantics","text":"Aspect Natural Language Code Flexibility Flexible, redundant, context-dependent Precise, rigid, context-sensitive Distribution of Meaning Distributed across words, sentences, and context Concentrated; each token can have a critical role Error Tolerance Minor errors (typos, word swaps) often ignored/understood Small changes (e.g., missing semicolon) can break code Ambiguity Common, resolved by context or intent Not tolerated; requires exactness"},{"location":"code_intelligence/code-generation-challenges/#distribution-and-impact-of-changes","title":"Distribution and Impact of Changes","text":"<ul> <li>Natural Language:</li> <li>Meaning is distributed; a single word rarely changes the entire message.</li> <li>Redundancy allows for graceful degradation\u2014messages are often recoverable.</li> <li> <p>Editing is forgiving; paraphrasing or rewording usually preserves intent.</p> </li> <li> <p>Code:</p> </li> <li>Meaning is concentrated; a single character can change program logic or cause failure.</li> <li>No redundancy\u2014every symbol matters.</li> <li>Editing is fragile; even minor changes can have cascading effects (syntax errors, logic bugs, security vulnerabilities).</li> </ul>"},{"location":"code_intelligence/code-generation-challenges/#challenges-for-llms","title":"Challenges for LLMs","text":"<ol> <li>Syntax Sensitivity:</li> <li>LLMs must generate code that is syntactically valid for the target language.</li> <li> <p>Minor mistakes can render code non-functional.</p> </li> <li> <p>Semantic Precision:</p> </li> <li>LLMs must understand the intent and context to generate correct logic.</li> <li> <p>Misunderstanding requirements can lead to subtle bugs.</p> </li> <li> <p>Context Management:</p> </li> <li>Code often depends on definitions and context spread across files or modules.</li> <li> <p>LLMs must track and respect scope, imports, and dependencies.</p> </li> <li> <p>Refactoring and Editing:</p> </li> <li>Editing code requires understanding dependencies and side effects.</li> <li> <p>LLMs must avoid introducing regressions when making changes.</p> </li> <li> <p>Testing and Validation:</p> </li> <li>Unlike natural language, code must be tested (compiled, run) to verify correctness.</li> <li>LLMs should ideally validate or simulate code execution.</li> </ol>"},{"location":"code_intelligence/code-generation-challenges/#why-these-challenges-matter","title":"Why These Challenges Matter","text":"<ul> <li>Reliability: Small errors can cause major failures in software systems.</li> <li>Safety: Bugs in code can lead to security vulnerabilities or data loss.</li> <li>Collaboration: Code is read and maintained by teams; clarity and correctness are essential.</li> </ul> <p>generated by janito.dev</p>"},{"location":"code_intelligence/code-generation-observability/","title":"Code Generation Observability","text":""},{"location":"code_intelligence/code-generation-observability/#overview","title":"Overview","text":"<p>Code Generation Observability is a feature that provides users with transparent, step-by-step visibility into the assistant's code search, analysis, and generation process. This allows users to:</p> <ul> <li>See which files, lines, and patterns are being searched.</li> <li>Observe the assistant's inference and workflow as it investigates issues or implements features.</li> <li>Intervene or redirect the assistant by providing feedback at each step, improving the quality and relevance of the results.</li> </ul>"},{"location":"code_intelligence/code-generation-observability/#benefits","title":"Benefits","text":"<ul> <li>Transparency: Users understand how results are produced.</li> <li>Debuggability: Easier to spot where misunderstandings or errors occur.</li> <li>Control: Users can guide the assistant more effectively.</li> </ul>"},{"location":"code_intelligence/code-generation-observability/#example-workflow","title":"Example Workflow","text":"<p>When investigating a problem (e.g., a missing keyboard shortcut), the assistant will:</p> <ol> <li>Search for relevant keywords or patterns in the codebase.</li> <li>Display search results, including file names and matching lines.</li> <li>Summarize findings and request user input if multiple directions are possible.</li> <li>Continue investigation or implementation based on user feedback.</li> </ol>"},{"location":"code_intelligence/code-generation-observability/#screenshot","title":"Screenshot","text":"<p>generated by janito.dev</p>"},{"location":"code_intelligence/our-approach/","title":"Our Approach to Code Intelligence and Editing","text":""},{"location":"code_intelligence/our-approach/#overview","title":"Overview","text":"<p>Janito\u2019s approach to code intelligence is designed to maximize transparency, traceability, and user alignment. We leverage methods and primitives that are familiar to developers and closely aligned with human inference and established tooling.</p>"},{"location":"code_intelligence/our-approach/#context-building-with-human-like-primitives","title":"Context Building with Human-like Primitives","text":"<ul> <li>Search Text &amp; Search File:</li> <li>We use explicit text and file search operations to build context, similar to how developers use <code>grep</code>, <code>find</code>, or IDE search.</li> <li>This makes every step visible and auditable, allowing users to understand and guide the assistant\u2019s inference.</li> </ul>"},{"location":"code_intelligence/our-approach/#string-replacement-over-diff","title":"String Replacement over Diff","text":"<ul> <li>Natural Language Alignment:</li> <li>Instead of relying solely on code diffs, we use string replacement primitives.</li> <li>This approach is more aligned with how humans describe changes (\"replace X with Y\"), and is easier to validate and review.</li> <li>It reduces ambiguity and makes the change process more transparent.</li> </ul>"},{"location":"code_intelligence/our-approach/#range-selection-and-contextual-references","title":"Range Selection and Contextual References","text":"<ul> <li>Filename:Line Number:Context:</li> <li>We adopt conventions like <code>filename:line_nr:context</code> for referencing code locations.</li> <li>This is inspired by tools like <code>grep</code> and error reporting systems in Python, JavaScript, and other languages.</li> <li>It enables precise, context-rich navigation and error reporting.</li> </ul>"},{"location":"code_intelligence/our-approach/#benefits-of-our-approach","title":"Benefits of Our Approach","text":"<ul> <li>Transparency: Every step is explicit and visible to the user.</li> <li>Traceability: Changes and inference can be audited and reviewed.</li> <li>User Alignment: Methods are familiar to developers, reducing friction and cognitive load.</li> <li>Reliability: By mirroring established developer workflows, we minimize surprises and errors.</li> </ul> <p>generated by janito.dev</p>"},{"location":"code_intelligence/why-string-replacement/","title":"Why Janito Prefers String\u2011Replacement Rules over Unified Diffs","text":""},{"location":"code_intelligence/why-string-replacement/#overview","title":"Overview","text":"<p>Janito is an LLM\u2011driven code\u2011editing agent. Instead of asking the model to provide a unified diff, Janito provides tooling primitives that steer the model to emit a set of deterministic plain\u2011string find/replace rules, which Janito then applies atomically. This choice maximises reliability, prompt economy, and alignment with the model\u2019s learned behaviour.</p>"},{"location":"code_intelligence/why-string-replacement/#1-trainingsignal-alignment","title":"1  Training\u2011Signal Alignment","text":"<ul> <li>Dominant exposure to raw code. In public\u2011code crawls, plain source lines outnumber diff tokens by ~20\u201140\u202f:\u202f1. The model has far richer \u201cmuscle memory\u201d for patterns like <code>foo(bar)</code> than for hunk headers such as <code>@@ -42,7 +42,8 @@</code>.</li> <li>Micro\u2011edit datasets reinforce replacements. Fine\u2011tune corpora like Google Codediffs and CommitPack present before/after snippets aligned token\u2011by\u2011token. The most common gradient update is \u201csubstitute X with Y,\u201d not \u201cparse and merge a patch.\u201d</li> <li>Pull\u2011request &amp; review corpora add contextual edits. Large crawls ingest GitHub PR diffs, mailing\u2011list patches (e.g., LKML), and Stack\u202fOverflow suggested edits. These sources boost the model\u2019s familiarity with diff syntax, but they remain a minority slice of the overall training mix and are often noisier than pristine source.</li> <li>Forums &amp; step\u2011by\u2011step tutorials reinforce direct replacements. Blog posts and Q&amp;A answers frequently present code \u201cbefore\u201d and \u201cafter,\u201d or instruct: \u201cChange <code>foo = false</code> to <code>foo = true</code> in your config.\u201d These snippets rarely include full diff headers; they mirror the granularity of plain string edits, further tuning the model toward replacement\u2011style transformations.</li> </ul> <p>Implication: A replacement rule asks the model to perform the transformation it has practised millions of times; interpreting a diff asks it to switch to a much rarer skill.</p>"},{"location":"code_intelligence/why-string-replacement/#2-tokenbudget-efficiency","title":"2  Token\u2011Budget Efficiency","text":"Expression of the same change Typical token cost Unified diff (6\u2011line hunk) ~70\u201390 tokens Plain string\u2011replacement rule ~10\u201315 tokens <p>Shorter prompts leave more room for actual code and high\u2011level instructions, reducing context\u2011window pressure and latency.</p>"},{"location":"code_intelligence/why-string-replacement/#3-robustness-in-real-codebases","title":"3  Robustness in Real Codebases","text":"<ul> <li>Line\u2011shift tolerance. If the file drifts after the diff was generated, context lines may no longer match. A string rule keyed to the target pattern still fires.</li> <li>Noise immunity. Email trailers, MIME boundaries, or CI banners embedded in patches confuse parsers but do not affect literal pattern matching.</li> <li>Encoding quirks. Different EOL conventions or charset mishaps break patch offsets; a plain string match usually survives them.</li> </ul>"},{"location":"code_intelligence/why-string-replacement/#takeaway","title":"Takeaway","text":"<p>Plain string\u2011replacement rules line up with the LLM\u2019s most frequent training examples, use a fraction of the tokens, and sidestep brittle patch\u2011parsing failure modes. That is why Janito\u2019s selected edit strategy is rule\u2011first.</p>"},{"location":"concepts/","title":"Concepts &amp; Terminology","text":"<p>This section collects foundational explanations, terminology, and conventions used throughout the documentation. It is intended to help new users and developers understand the key ideas and language model concepts relevant to Janito and similar tools.</p>"},{"location":"concepts/#available-topics","title":"Available Topics","text":"<ul> <li> <p>What Is a Language Model Client?</p> </li> <li> <p>Prompt Analysis Style</p> </li> <li>Prompt Design Style</li> </ul> <p>More topics will be added here as the documentation evolves.</p>"},{"location":"concepts/analysis-style/","title":"Analysis Prompting Style: Declaring Role and Knowledge Domain","text":""},{"location":"concepts/analysis-style/#overview","title":"Overview","text":"<p>For effective and reliable AI-driven analysis, prompts should begin by explicitly declaring the intended role and the relevant knowledge domain. This establishes context, sets expectations, and guides the model\u2019s reasoning and language style.</p>"},{"location":"concepts/analysis-style/#why-declare-role-and-domain","title":"Why Declare Role and Domain?","text":"<ul> <li>Role Declaration: Instructs the model to adopt a specific perspective (e.g., software engineer, security auditor, data scientist).</li> <li>Domain Declaration: Focuses the model\u2019s attention on the relevant field or subject matter (e.g., Python projects, web security, machine learning).</li> </ul> <p>Explicitly stating both helps: - Reduce ambiguity - Improve relevance and accuracy - Align output with user intent</p>"},{"location":"concepts/analysis-style/#example-structure","title":"Example Structure","text":"<pre><code>You are a(n) [role] with expertise in [domain]. Your task is to...\n</code></pre>"},{"location":"concepts/analysis-style/#example-prompts","title":"Example Prompts","text":"<ul> <li>You are an expert software project analyst. Your task is to analyze the provided project files and identify the core technologies used in the project.</li> <li>You are a security auditor specializing in web applications. Review the configuration files for potential vulnerabilities.</li> <li>You are a data scientist with experience in time series analysis. Examine the dataset and summarize key trends.</li> </ul>"},{"location":"concepts/analysis-style/#extending-with-analysis-actions","title":"Extending with Analysis Actions","text":"<p>After declaring the role and domain, extend the prompt with clear, actionable analysis instructions. </p>"},{"location":"concepts/analysis-style/#guidance","title":"Guidance:","text":"<ul> <li>Clearly state the analysis objective (e.g., \"identify core technologies\", \"summarize vulnerabilities\", \"extract key metrics\").</li> <li>Specify the expected output format (e.g., bullet points, summary, table).</li> <li>Avoid ambiguity\u2014list exactly what should be included or excluded.</li> <li>Use the condition-before-action (CBA) structure for any prerequisites or constraints.</li> </ul>"},{"location":"concepts/analysis-style/#best-practices","title":"Best Practices","text":"<ul> <li>Always start with a clear role and domain statement.</li> <li>Extend with explicit analysis actions and output requirements.</li> <li>Use precise, unambiguous language.</li> <li>Follow with condition-before-action (CBA) structure for instructions (see Prompt Design Style).</li> <li>Avoid vague roles (e.g., \"expert\"). Specify the field or context when possible.</li> </ul> <p>generated by janito.dev</p>"},{"location":"concepts/language-model-clients/","title":"What Is a Language Model Client?","text":""},{"location":"concepts/language-model-clients/#what-is-a-language-model-client","title":"What Is a Language Model Client?","text":"<p>A Language Model client is a software component or application that interacts with a language model via a RESTful API. The client sends requests over HTTP(S), supplying a prompt and optional parameters, and then processes the response returned by the service. This architecture abstracts away the complexities of model hosting, scaling, and updates, allowing developers to focus on application logic.</p>"},{"location":"concepts/language-model-clients/#thin-vs-thick-clients","title":"Thin vs. Thick Clients","text":"<p>Language Model clients generally fall into two categories based on where and how much processing they handle: Thin Clients and Thick Clients.</p>"},{"location":"concepts/language-model-clients/#thin-clients","title":"Thin Clients","text":"<p>A thin client is designed to be lightweight and stateless. It primarily acts as a straightforward conduit that relays user prompts and parameters directly to the language model service and passes the raw response back to the application, similar to how a remote control sends commands without processing them. Key characteristics include:</p> <ul> <li>Minimal Processing: Performs little to no transformation on the input prompt or the output response beyond basic formatting and validation.</li> <li>Low Resource Usage: Requires minimal CPU and memory, making it easy to deploy in resource-constrained environments like IoT devices or edge servers.</li> <li>Model Support: Supports both small-footprint models (e.g., <code>*-mini</code>, <code>*-nano</code>) for low-latency tasks and larger models (e.g., GPT O3 Pro, Sonnet 4 Opus) when higher accuracy or more complex reasoning is required.</li> <li>Agentic Capabilities: Supports function calls for agentic workflows, enabling dynamic tool or API integrations that allow the client to perform actions based on LLM responses.</li> <li>Ease of Maintenance: Simple codebase with few dependencies, leading to easier updates and debugging.</li> <li>Self-Sufficiency: Can operate independently without bundling additional applications, ideal for lightweight deployments.</li> </ul> <p>Use Case: A CLI code assistant like aider.chat or janito.dev, which runs as a command-line tool, maintains session context, refines developer prompts, handles fallbacks, and integrates with local code repositories before sending requests to the LLM and processing responses for display in the terminal.</p>"},{"location":"concepts/language-model-clients/#thick-clients","title":"Thick Clients","text":"<p>A thick client handles more logic locally before and after communicating with the LLM service. It may preprocess prompts, manage context, cache results, or post-process responses to enrich functionality. Key characteristics include:</p> <ul> <li>Higher Resource Usage: Requires more CPU, memory, and possibly GPU resources, as it performs advanced processing locally.</li> <li>Model Requirements: Typically designed to work with larger, full-weight models (e.g., GPT-4, Llama 65B), leveraging richer capabilities at the cost of increased latency and resource consumption.</li> <li>Enhanced Functionality: Offers capabilities like local caching for rate limiting, advanced analytics on responses, or integration with other local services (e.g., databases, file systems).</li> <li>Inter-Client Communication: Supports Model Context Protocol (MCP) or Agent-to-Agent (A2A) workflows, enabling coordination and task delegation among multiple agent instances.</li> <li>Bundled Integration: Often bundled or coupled with desktop or web applications to provide a richer user interface and additional features.</li> </ul> <p>Use Case: A desktop application that manages multi-turn conversations, maintains state across sessions, and integrates user-specific data before sending refined prompts to the LLM and processing the returned content for display.</p> <p>Next, we can explore considerations such as security, scaling, and best practices for choosing between thin and thick clients.</p>"},{"location":"concepts/prompt-design-style/","title":"Prompt Design Style: Condition Before Action","text":""},{"location":"concepts/prompt-design-style/#a-key-ordering-principle-in-language-and-prompt-engineering","title":"A Key Ordering Principle in Language and Prompt Engineering","text":"<p>In both natural language and prompt engineering, the structure and order of words significantly impact clarity and effectiveness. One notable pattern is the presentation of a condition before the subsequent action\u2014commonly known as the condition before action order. This article explores the prevalence and importance of this structure, especially in contexts where precise instructions or prompts are required.</p>"},{"location":"concepts/prompt-design-style/#what-does-condition-before-action-mean","title":"What Does Condition Before Action Mean?","text":"<p>The condition before action structure is when a statement specifies a prerequisite or context (the condition) prior to describing the main step or activity (the action). For example:</p> <ul> <li>Condition before action: Before removing or renaming files, update all references and validate the relevant aspects of the system.</li> <li>Action before condition: Update all references and validate the relevant aspects of the system before removing or renaming files.</li> </ul> <p>While both structures can be grammatically correct and convey the intended meaning, the former more explicitly signals to the reader or listener that fulfillment of the condition must precede the action. This is particularly valuable in technical writing, safety protocols, and instructions that must be followed precisely.</p>"},{"location":"concepts/prompt-design-style/#linguistic-perspective","title":"Linguistic Perspective","text":"<p>From a linguistic standpoint, fronting the condition is a way to foreground critical context. This satisfies a reader's expectation for information sequence: context first, then the result or necessary action. Linguists often refer to this as maintaining logical and temporal coherence, which is essential to effective communication.</p>"},{"location":"concepts/prompt-design-style/#implications-for-prompt-engineering","title":"Implications for Prompt Engineering","text":"<p>Prompt engineering\u2014the art of crafting effective inputs for large language models (LLMs)\u2014relies on linguistic patterns present in training corpora. Because much of the high-quality material these models learn from (technical documentation, instructions, programming guides) uses condition before action ordering, LLMs are more likely to interpret and execute prompts that follow this structure accurately.</p> <p>For example, prompting an LLM with:</p> <p>Before you create the report, ensure the data is validated.</p> <p>provides a clear sequence, reducing ambiguity compared to:</p> <p>Ensure the data is validated before you create the report.</p> <p>While LLMs can process both forms, explicit and sequential phrasing aligns better with their linguistic training and often yields more reliable results.</p>"},{"location":"concepts/prompt-design-style/#why-order-matters","title":"Why Order Matters","text":"<p>Generalizing beyond just condition before action, order-of-words is a critical factor in communicating instructions, expressing logic, and minimizing misunderstandings. Other important orders include:</p> <ul> <li>Cause before effect: Because the file was missing, the build failed.</li> <li>Reason before request: Since you're available, could you review this?</li> <li>Qualifier before command: If possible, finish this by noon.</li> </ul> <p>Each of these helps set context and prevent errors\u2014essential in instructive writing and conversational AI interactions.</p>"},{"location":"concepts/prompt-design-style/#avoiding-ambiguity-be-explicit-with-actions-and-objects","title":"Avoiding Ambiguity: Be Explicit with Actions and Objects","text":"<p>A common source of ambiguity in prompts is the use of vague verbs such as \"validate\", \"check\", or \"review\" without specifying what is being validated, checked, or reviewed, and by what criteria. For example, the instruction \"validate the system\" is ambiguous: what aspects of the system should be validated, and how?</p>"},{"location":"concepts/prompt-design-style/#guideline","title":"Guideline:","text":"<ul> <li>Avoid vague verbs without a clear object and criteria. Instead, specify what should be validated and how. For example, use \"validate the relevant configuration files for syntax errors\" or \"validate the output matches the expected format\".</li> <li>When using the condition-before-action structure, ensure both the condition and the action are explicit and unambiguous.</li> </ul>"},{"location":"concepts/prompt-design-style/#example-generalized","title":"Example (generalized):","text":"<ul> <li>Ambiguous: Before removing or renaming files, validate the system.</li> <li>Improved: Before removing or renaming files, validate the relevant aspects of the system (e.g., configuration, dependencies, and references).</li> </ul>"},{"location":"concepts/prompt-design-style/#note","title":"Note:","text":"<p>The phrase \"validate the system before removing or renaming files\" does follow the condition-before-action structure, but the object (\"the system\") should be made more explicit for clarity and reliability.</p>"},{"location":"concepts/prompt-design-style/#qualifiers-determinism-and-llm-behavior","title":"Qualifiers, Determinism, and LLM Behavior","text":""},{"location":"concepts/prompt-design-style/#are-always-and-never-conditions","title":"Are \"Always\" and \"Never\" Conditions?","text":"<p>Words like \"Always\" and \"Never\" are absolute qualifiers, not true conditions. While they may appear to set clear, deterministic boundaries, their interpretation by large language models (LLMs) is not guaranteed to be consistent. LLMs operate probabilistically, so even instructions with absolute qualifiers can yield unexpected or inconsistent results.</p>"},{"location":"concepts/prompt-design-style/#are-qualifiers-ambiguous","title":"Are Qualifiers Ambiguous?","text":"<p>Qualifiers such as \"if possible,\" \"always,\" or \"never\" can introduce ambiguity, especially in the context of LLMs. While these words are often clear to humans, LLMs may interpret or prioritize them differently depending on context, training data, and prompt structure. This means that even deterministic-sounding qualifiers may not produce deterministic outcomes.</p>"},{"location":"concepts/prompt-design-style/#preferred-strategies-for-prompt-engineering","title":"Preferred Strategies for Prompt Engineering","text":"<p>Given the non-deterministic, probabilistic nature of LLMs, it is advisable to: - Prefer explicit, context-setting conditions (e.g., \"Before you do X, ensure Y\") over absolute or vague modifiers. - Avoid relying solely on words like \"always\" or \"never\" to enforce strict behavior. - Structure prompts to minimize ambiguity and maximize clarity, aligning with the sequential logic that LLMs are most likely to follow reliably.</p> <p>This approach reduces the risk of unexpected results and improves the reliability of LLM outputs.</p>"},{"location":"concepts/prompt-design-style/#conclusion","title":"Conclusion","text":"<p>Whether you're writing documentation, crafting conversational prompts for AI, or giving instructions, placing conditions before actions is an effective way to convey clear, sequential logic. Not only does this habit align with natural linguistic expectations, but it also optimizes your communication for language models trained on human language patterns. In both human communication and AI prompting, condition before action is a foundational principle that promotes understanding and successful outcomes.</p> <p>generated by janito.dev</p>"},{"location":"drivers/events/","title":"Events","text":"<p>\u2022 text: Plain text (can also be code).  \u2022 inline_data: Inlined bytes data (e.g., binary blobs).  \u2022 file_data: URI-based data (e.g., a file reference).  \u2022 video_metadata: Metadata for a video.  \u2022 code_execution_result: The result of executing code (stdout, stderr, etc.).  \u2022 executable_code: Code generated by the model, meant to be executed.  \u2022 function_call: A predicted function call (name and arguments).  \u2022 function_response: The result/output of a function call.  \u2022 thought: A boolean flag indicating if the part is a \"thought\" from the model.</p>"},{"location":"guides/configuration/","title":"Configuration Guide","text":"<p>Janito can be configured using command-line options, environment variables, or configuration files. This guide shows you how to set up API keys, select providers and models, and adjust other settings.</p>"},{"location":"guides/configuration/#1-command-line-options-recommended-for-most-users","title":"1. Command-Line Options (Recommended for Most Users)","text":"<p>Set API keys, providers, and models directly when running Janito:</p> <pre><code>janito --set-api-key YOUR_API_KEY -p PROVIDER_NAME\njanito --set provider=openai\njanito -p openai -m gpt-3.5-turbo \"Your prompt here\"\n</code></pre> <ul> <li>Use <code>-p PROVIDER_NAME</code> to select a provider.</li> <li>Use <code>-m MODEL_NAME</code> to select a model for the provider.</li> <li>See CLI Options for the full list of flags.</li> </ul>"},{"location":"guides/configuration/#3-configuration-file","title":"3. Configuration File","text":"<p>Janito uses a <code>config.json</code> file located in the <code>.janito</code> directory under your home folder for persistent settings.</p> <p>Path:</p> <ul> <li>Windows: <code>C:\\Users\\&lt;YourUser&gt;\\.janito\\config.json</code></li> <li>Linux/macOS: <code>/home/&lt;youruser&gt;/.janito/config.json</code></li> </ul> <p>You can edit this file directly or use Janito CLI commands to update your configuration.</p>"},{"location":"guides/configuration/#viewing-effective-configuration","title":"Viewing Effective Configuration","text":"<p>Show the current configuration with:</p> <pre><code>janito --show-config\n</code></pre>"},{"location":"guides/configuration/#more-information","title":"More Information","text":"<ul> <li>See CLI Options Reference for all configuration flags.</li> <li>For provider-specific settings, see the Supported Providers &amp; Models page.</li> <li>For troubleshooting, use <code>janito --help</code> or consult the Usage Guide.</li> </ul>"},{"location":"guides/developing/","title":"Developing &amp; Extending Janito","text":"<p>This guide explains how to set up Janito for development and install the latest version from GitHub.</p>"},{"location":"guides/developing/#installing-the-latest-development-version","title":"Installing the Latest Development Version","text":"<p>To install the most recent development version from the GitHub main branch, run:</p> <pre><code>pip install git+https://github.com/janito-dev/janito.git@main\n</code></pre>"},{"location":"guides/developing/#editable-install-for-local-development","title":"Editable Install for Local Development","text":"<p>To make code changes and see them reflected immediately (without reinstalling), use an editable install:</p> <pre><code>git clone https://github.com/janito-dev/janito.git\ncd janito\ngit checkout main\npip install -e .\n</code></pre> <p>This installs Janito in \"editable\" mode, so changes to the source code are instantly available in your environment.</p>"},{"location":"guides/developing/#additional-development-setup","title":"Additional Development Setup","text":"<ul> <li>Ensure you are on the correct branch (e.g., <code>main</code>) for the latest development version.</li> <li>For linting, pre-commit hooks, and other developer tools, see the Developer Toolchain Guide in the meta directory.</li> </ul>"},{"location":"guides/installation/","title":"Installation Guide","text":"<p>This guide explains how to install Janito and verify your setup.</p>"},{"location":"guides/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or newer</li> </ul>"},{"location":"guides/installation/#installation-methods","title":"Installation Methods","text":"<p>You can install Janito using pip from either PyPI (for stable releases) or directly from GitHub (for the latest development version).</p>"},{"location":"guides/installation/#from-pypi-stable","title":"From PyPI (Stable)","text":"<pre><code>pip install janito\n</code></pre>"},{"location":"guides/installation/#from-github-development-version","title":"From GitHub (Development Version)","text":"<pre><code>pip install git+https://github.com/janito-dev/janito.git\n</code></pre> <p>For development setup and contributing, see Developing &amp; Extending.</p>"},{"location":"guides/installation/#verifying-your-installation","title":"Verifying Your Installation","text":"<p>To confirm Janito is installed correctly, run:</p> <pre><code>janito --help\n</code></pre> <p>You should see the Janito CLI help message.</p>"},{"location":"guides/installation/#related-guides","title":"Related Guides","text":"<ul> <li>Configuration Guide</li> <li>Usage Guide</li> <li>Developing &amp; Extending</li> </ul>"},{"location":"guides/single-shot-terminal/","title":"\u26a1 One-Shot Terminal","text":"<p>If you want to run a single command or prompt in the terminal shell without entering interactive mode, you can use the \"one-shot\" feature. This is useful for quick tasks or scripting.</p> <p>To use one-shot mode, simply provide your prompt as a command-line argument:</p> <pre><code>janito \"what are the key classes of this project?\"\n</code></pre> <p>Janito will process your request and exit after displaying the result.</p> <p>\u26a0\ufe0f Warning: Some models may not complete all required steps in a single-shot (one-off) run. If the model does not act as expected, try appending \"; just do it\" to your command-line prompt to encourage direct action.</p> <p></p> <p>generated by janito.dev</p>"},{"location":"guides/terminal-shell/","title":"\ud83d\udda5\ufe0f Terminal Shell (Interactive Mode)","text":"<p>The interactive shell lets you have a continuous conversation with Janito, just like chatting with a smart assistant. This mode is ideal for deep dives, brainstorming, or when you want to iteratively refine your requests.</p>"},{"location":"guides/terminal-shell/#features","title":"\u2728 Features","text":"<ul> <li>\ud83d\udd04 Multi-turn conversations: Build on previous answers and context</li> <li>\u2b06\ufe0f\u2b07\ufe0f Command history: Use the up/down arrows to revisit previous prompts</li> <li>\ud83c\udfa8 Syntax highlighting for code responses</li> <li>\ud83d\udccb Copy code snippets easily</li> <li>\ud83d\udca1 Context-aware suggestions (where supported)</li> <li>\ud83d\udcbe Conversation state is saved/restored between sessions</li> <li>\ud83d\uddb1\ufe0f Clickable file links: Click on file names in responses to open them in a web viewer for detailed inspection and actions (see below)</li> </ul>"},{"location":"guides/terminal-shell/#built-in-commands","title":"\ud83d\udcdd Built-in Commands","text":"<p>You can use these commands at any time (prefix with <code>/</code> or just type the name):</p> Command Description <code>/exit</code>, <code>exit</code> Exit chat mode <code>/restart</code> Restart the CLI <code>/help</code> Show help message with available commands <code>/restart</code> Reset conversation history / start new task <code>/continue</code> Restore last saved conversation <code>/restart</code> Reset conversation history <code>/history [N]</code> Show last N messages (default: 5) <code>/prompt</code> Show the current system prompt <code>/role &lt;description&gt;</code> Change the system role (e.g., \"You are a code reviewer\") <code>/lang &lt;code&gt;</code> Change the interface language (e.g., <code>/lang pt</code>, <code>/lang en</code>) <code>/clear</code> Clear the terminal screen <code>/multi</code> Enter multiline input mode (write multi-line text, Esc+Enter) <code>/config</code> Show or set configuration (see: <code>/config show</code>, <code>/config set local|global key=value</code>)"},{"location":"guides/terminal-shell/#clickable-file-links","title":"\ud83d\uddb1\ufe0f Clickable File Links","text":"<p>When Janito references files in its responses, the file names may appear as clickable links in supported terminals or web-based shells. To enable this, start Janito with the <code>-w</code> or <code>--web</code> flag. Clicking these links will open the file in the lightweight web file viewer (powered by <code>janito/termweb</code>, a Quart-based web file viewer).</p> <ul> <li>What you can do:</li> <li>View file contents in your browser</li> <li>Access additional actions (e.g., copy, download, or inspect details)</li> <li>Navigate project files more efficiently during reviews or debugging</li> </ul> <p>This feature enhances productivity by bridging the terminal and browser, making it easy to inspect and act on files referenced in your conversations.</p>"},{"location":"guides/terminal-shell/#usage-example","title":"\ud83d\udcbb Usage Example","text":"<pre><code>janito\n</code></pre> <p>You\u2019ll be dropped into a conversational prompt where you can interact with Janito step by step. Type <code>/help</code> to see available commands at any time. Use <code>/restart</code> to start a new task or reset context.</p> <p></p> <p>Screenshot: Janito interactive shell in action</p> <p>generated by janito.dev</p>"},{"location":"guides/tools-developer-guide/","title":"Tools Developer Guide","text":"<p>This guide explains how to add a new tool (functionality) to Janito so it can be used by the agent and OpenAI-compatible APIs.</p> <p>For a list of all built-in tools and their usage, see the Tools Reference. For a technical overview, see the Architecture Guide in the documentation navigation.</p>"},{"location":"guides/tools-developer-guide/#requirements","title":"Requirements","text":"<ul> <li>Class-based tools: Implement tools as classes inheriting from <code>ToolBase</code> (see <code>janito/agent/tool_base.py</code>).</li> <li>Type hints: All parameters to the <code>run</code> method must have Python type hints.</li> <li>Docstrings:</li> <li>The tool class must have a class-level docstring summarizing its purpose and behavior (user-facing).</li> <li>The <code>run</code> method must have a Google-style docstring with an <code>Args:</code> section describing each parameter.</li> <li>Parameter descriptions: Every parameter must have a corresponding description in the docstring. If any are missing, registration will fail.</li> </ul>"},{"location":"guides/tools-developer-guide/#example-creating-a-tool","title":"Example: Creating a Tool","text":"<pre><code>from janito.agent.tool_base import ToolBase\nfrom janito.agent.tool_registry import register_tool\n\n@register_tool\nclass MyTool(ToolBase):\n    name = \"my_tool\"\n    \"\"\"\n    Processes a file a given number of times.\n    \"\"\"\n\n    def run(self, filename: str, count: int) -&gt; None:\n        \"\"\"\n        Processes the specified file repeatedly.\n\n        Args:\n            filename (str): The path to the file to process.\n            count (int): How many times to process the file.\n        \"\"\"\n        # Implementation here\n</code></pre>"},{"location":"guides/tools-developer-guide/#steps-to-add-a-tool","title":"Steps to Add a Tool","text":"<ol> <li>Define your tool as a class inheriting from <code>ToolBase</code>.</li> <li>Add a class-level docstring summarizing the tool's purpose (user-facing).</li> <li>Implement the <code>run</code> method with type hints and a Google-style docstring, including an <code>Args:</code> section for every parameter.</li> <li>Register your tool with <code>@register_tool</code> from <code>janito.agent.tool_registry</code>. Set a unique class attribute <code>name = \"your_tool_name\"</code>.</li> <li>Document your tool: Update <code>janito/agent/tools/README.md</code> with a short description and usage for your new tool.</li> </ol>"},{"location":"guides/tools-developer-guide/#docstring-style","title":"Docstring Style","text":"<p>Use the Google style for docstrings:</p> <pre><code>\"\"\"\nFunction summary.\n\nArgs:\n    param1 (type): Description of param1.\n    param2 (type): Description of param2.\n\"\"\"\n</code></pre> <ul> <li>The <code>Args:</code> section must list each parameter, its type, and a description.</li> <li>The class docstring is prepended to the tool's description in the OpenAI schema and is user-facing.</li> </ul>"},{"location":"guides/tools-developer-guide/#what-happens-if-you-omit-a-description","title":"What Happens If You Omit a Description?","text":"<p>If you forget to document a parameter, you will see an error like:</p> <pre><code>ValueError: Parameter 'count' in tool 'MyTool' is missing a description in the docstring.\n</code></pre>"},{"location":"guides/tools-developer-guide/#tool-reference","title":"Tool Reference","text":"<p>See the Tools Reference page in the documentation navigation for a list of built-in tools and their usage.</p>"},{"location":"guides/tools-developer-guide/#tool-call-limits","title":"Tool Call Limits","text":"<p>You can use <code>--max-tools</code> to limit the total number of tool runs allowed in a chat session. If the limit is reached, further tool runs will be prevented.</p>"},{"location":"guides/tools-developer-guide/#system-prompt-precedence","title":"System Prompt Precedence","text":"<ul> <li>If <code>--system-file</code> is provided, the file's content is used as the system prompt (highest priority).</li> <li>Otherwise, if <code>--system</code> or the config value is set, that string is used.</li> <li>Otherwise, a default prompt is used from the template at <code>janito/agent/templates/prompt_prompt_template.j2</code>.</li> </ul>"},{"location":"guides/tools-developer-guide/#interactive-shell-config-commands","title":"Interactive Shell Config Commands","text":"<p>Within the interactive chat shell, you can use special commands: - <code>/config show</code> \u2014 Show effective configuration (local, global, defaults) - <code>/config set local key=value</code> \u2014 Set a local config value - <code>/config set global key=value</code> \u2014 Set a global config value - <code>/continue</code> \u2014 Restore the last saved conversation - <code>/start</code> \u2014 Reset conversation history - <code>/prompt</code> \u2014 Show the current system prompt - <code>/help</code> \u2014 Show help message</p>"},{"location":"guides/tools-developer-guide/#summary","title":"Summary","text":"<ul> <li>Implement tools as classes inheriting from <code>ToolBase</code>.</li> <li>Provide type hints and parameter descriptions for the <code>run</code> method.</li> <li>Use Google-style docstrings for both the class and the <code>run</code> method.</li> <li>Registration will fail if any parameter is undocumented.</li> <li>Update the tools README after adding a new tool.</li> </ul>"},{"location":"guides/using/","title":"Using Janito: Quickstart &amp; Basic Usage","text":"<p>This guide explains how to start using Janito after installation. For an overview, see the Introduction. For setup, see the Installation Guide and Configuration Guide.</p>"},{"location":"guides/using/#quickstart","title":"Quickstart","text":"<p>After installing Janito, you can use it from the command line or launch the web interface:</p>"},{"location":"guides/using/#run-a-one-off-prompt","title":"Run a One-Off Prompt","text":"<pre><code>janito \"Refactor the data processing module to improve readability.\"\n</code></pre>"},{"location":"guides/using/#start-the-interactive-chat-shell","title":"Start the Interactive Chat Shell","text":"<pre><code>janito\n</code></pre> <p>Or, to enable clickable file links in your browser during the session:</p> <pre><code>janito -w\n</code></pre>"},{"location":"guides/using/#launch-the-web-terminal-interface","title":"Launch the Web Terminal Interface","text":"<pre><code>janito termweb\n</code></pre>"},{"location":"guides/using/#basic-usage-tips","title":"Basic Usage Tips","text":"<ul> <li>Use natural language to describe what you want Janito to do (e.g., \"Add type hints to all functions in utils.py\").</li> <li>In the chat shell, use <code>/help</code> for available commands.</li> <li>Use CLI flags to customize behavior (see CLI Options).</li> </ul>"},{"location":"guides/using/#more-resources","title":"More Resources","text":"<ul> <li>How Janito Uses Tools: Automatic tool selection details.</li> <li>Supported Models: See documentation navigation for LLM compatibility.</li> <li>Costs &amp; Value Transparency: Pricing and efficiency details.</li> </ul>"},{"location":"guides/using_tools/","title":"\ud83d\udee0\ufe0f How Janito Uses Tools","text":"<p>Janito is designed to work for you automatically. When you ask a question or make a request, Janito selects and uses the most relevant tools behind the scenes\u2014no manual setup required.</p>"},{"location":"guides/using_tools/#why-list-the-tools","title":"\ud83d\udc40 Why List the Tools?","text":"<p>The following tools are listed for transparency and to help you understand how Janito works. You don\u2019t need to invoke them directly; Janito chooses the right tool based on your prompt and the current context.</p>"},{"location":"guides/using_tools/#types-of-tools","title":"\ud83e\uddf0 Types of Tools","text":"<ul> <li>\ud83d\udd0d File Search &amp; Content Extraction: Janito can search for files, read their contents, and extract relevant code or documentation.</li> <li>\ud83c\udfd7\ufe0f Code Outline &amp; Structure: Tools analyze Python files to provide outlines of classes, functions, and methods.</li> <li>\u2699\ufe0f Configuration &amp; Environment: Janito can inspect and report on project configuration files and runtime settings.</li> <li>\ud83c\udf10 Web Fetching: Some tools fetch and parse web content for research or documentation purposes.</li> <li>\ud83c\udfa8 Rich Output: Tools format and present results in a readable, actionable way (e.g., code blocks, tables).</li> </ul>"},{"location":"guides/using_tools/#full-reference","title":"\ud83d\udcd6 Full Reference","text":"<p>For a detailed list of all available tools and their options, see the Tools Reference.</p>"},{"location":"guides/prompting/","title":"Prompting Guide","text":"<p>This section contains detailed guides and best practices for prompt engineering, system prompt design, and domain-specific prompting strategies for Janito.</p>"},{"location":"guides/prompting/#contents","title":"Contents","text":"<ul> <li>Prompt Design Style: Condition Before Action</li> <li>(Add more guides for specific instructions, roles, and domains)</li> </ul> <p>This directory is intended for all documentation related to prompt construction, style, and advanced usage.</p>"},{"location":"meta/developer-toolchain/","title":"Developer Toolchain Guide","text":"<p>For tool development, see the Tools Developer Guide.</p>"},{"location":"meta/developer-toolchain/#code-style-linting-and-pre-commit-hooks","title":"Code Style, Linting, and Pre-commit Hooks","text":"<p>This project uses pre-commit to enforce code style and linting automatically using Black (formatter) and Ruff (linter).</p>"},{"location":"meta/developer-toolchain/#setup","title":"Setup","text":"<ol> <li>Install pre-commit if you haven't already:</li> </ol> <pre><code>pip install pre-commit\n</code></pre> <ol> <li>Install the hooks:</li> </ol> <pre><code>pre-commit install\n</code></pre>"},{"location":"meta/developer-toolchain/#usage","title":"Usage","text":"<ul> <li>Hooks will run automatically on <code>git commit</code>.</li> <li>To manually check all files:</li> </ul> <pre><code>pre-commit run --all-files\n</code></pre> <ul> <li>If any issues are found, pre-commit will attempt to fix them or display errors to resolve.</li> </ul>"},{"location":"meta/developer-toolchain/#notes","title":"Notes","text":"<ul> <li>Always run the hooks before pushing code to ensure consistent style and linting.</li> <li>See <code>.pre-commit-config.yaml</code> for configuration details.</li> </ul> <p>generated by janito.dev</p>"},{"location":"reference/api/","title":"API Reference","text":"<p>Welcome to the API reference for this project. This document provides an overview of the main classes and their locations within the codebase. Use this as a starting point for understanding and extending the core functionality.</p>"},{"location":"reference/api/#core-modules-and-main-classes","title":"Core Modules and Main Classes","text":""},{"location":"reference/api/#janitoagentconfig","title":"janito.agent.config","text":"<ul> <li>SingletonMeta: Metaclass for singleton pattern.</li> <li>BaseConfig: Base configuration class.</li> <li>FileConfig: File-based configuration.</li> <li>EffectiveConfig: Represents the effective configuration.</li> </ul>"},{"location":"reference/api/#janitoagentconversation","title":"janito.agent.conversation","text":"<ul> <li>ConversationHandler: Manages conversation state and flow.<ul> <li><code>__init__</code>: Initializes the handler with configuration and state.</li> <li><code>handle_conversation</code>: Processes a conversation turn and updates state.</li> <li><code>api_call</code>: Makes a direct API call for conversation handling.</li> </ul> </li> </ul>"},{"location":"reference/api/#janitoagentconversation_exceptions","title":"janito.agent.conversation_exceptions","text":"<ul> <li>MaxRoundsExceededError: Raised when conversation round limit is exceeded.</li> <li>EmptyResponseError: Raised when no response is generated.</li> <li>ProviderError: Raised for provider-specific errors.</li> </ul>"},{"location":"reference/api/#janitoagentmessage_handler","title":"janito.agent.message_handler","text":"<ul> <li>QueueMessageHandler: Handles message queuing.</li> </ul>"},{"location":"reference/api/#janitoagentopenai_client","title":"janito.agent.openai_client","text":"<ul> <li>Agent: Main agent class for OpenAI integration.<ul> <li><code>__init__</code>: Initializes the agent with model and API settings.</li> <li><code>chat</code>: Sends a list of messages to the LLM and returns the response.</li> <li><code>usage_history</code>: Tracks and returns usage statistics.</li> </ul> </li> </ul>"},{"location":"reference/api/#janitoagentprofile_manager","title":"janito.agent.profile_manager","text":"<ul> <li>AgentProfileManager: Manages agent profiles and prompt templates (currently only the \"base\" profile is supported).<ul> <li><code>__init__</code>: Loads and initializes profile data.</li> <li><code>set_role</code>: Sets the current agent role.</li> <li><code>render_prompt</code>: Renders the prompt template for the agent.</li> <li><code>refresh_prompt</code>: Reloads and refreshes the prompt template.</li> </ul> </li> </ul>"},{"location":"reference/api/#janitoagentqueued_message_handler","title":"janito.agent.queued_message_handler","text":"<ul> <li>QueuedMessageHandler: Handles queued messages.</li> </ul>"},{"location":"reference/api/#janitoagentrich_live","title":"janito.agent.rich_live","text":"<ul> <li>LiveMarkdownDisplay: Displays live markdown output.</li> </ul>"},{"location":"reference/api/#janitoagentrich_message_handler","title":"janito.agent.rich_message_handler","text":"<ul> <li>RichMessageHandler: Handles rich message formatting.</li> </ul>"},{"location":"reference/api/#janitoagentruntime_config","title":"janito.agent.runtime_config","text":"<ul> <li>RuntimeConfig: Runtime configuration derived from BaseConfig.</li> <li>UnifiedConfig: Unified configuration interface.</li> </ul>"},{"location":"reference/api/#janitoagenttool_base","title":"janito.agent.tool_base","text":"<ul> <li>ToolBase: Abstract base class for all tools.</li> </ul>"},{"location":"reference/api/#tool-implementations-janitoagenttools","title":"Tool Implementations (janito.agent.tools)","text":"<p>Each tool inherits from <code>ToolBase</code> and implements a specific function:</p> <ul> <li>AskUserTool</li> <li>CreateDirectoryTool</li> <li>CreateFileTool</li> <li>FetchUrlTool</li> <li>FindFilesTool</li> <li>GetFileOutlineTool</li> <li>ViewFileTool</li> <li>StoreMemoryTool</li> <li>RetrieveMemoryTool</li> <li>MoveFileTool</li> <li>PyCompileFileTool</li> <li>RemoveDirectoryTool</li> <li>RemoveFileTool</li> <li>ReplaceFileTool</li> <li>ReplaceTextInFileTool</li> <li>RunBashCommandTool</li> <li>RunPythonCommandTool</li> <li>SearchFilesTool</li> </ul>"},{"location":"reference/api/#cli-and-web-interfaces","title":"CLI and Web Interfaces","text":""},{"location":"reference/api/#janitocli","title":"janito.cli","text":"<ul> <li>CLI entry points and utilities for command-line usage.</li> </ul>"},{"location":"reference/api/#janitotermwebapp","title":"janito.termweb.app","text":"<ul> <li>Quart-based web application entry point for file viewing and browser-based interaction. Provides the main web interface for interacting with the agent and tools via HTTP, and enables clickable file links in CLI environments.</li> </ul> <p>For detailed class and method documentation, see the source code or future expanded API docs.</p>"},{"location":"reference/api/#generated-by-janitodev","title":"generated by janito.dev","text":""},{"location":"reference/azure-openai/","title":"Using Azure OpenAI with Janito","text":"<p>Janito supports models hosted on Azure OpenAI in addition to OpenAI-compatible endpoints.</p>"},{"location":"reference/azure-openai/#configuration-steps","title":"Configuration Steps","text":"<ol> <li> <p>Set your Azure OpenAI endpoint:    Set the <code>base_url</code> to your Azure OpenAI endpoint, for example:    <code>https://YOUR-RESOURCE-NAME.openai.azure.com/openai/deployments/YOUR-DEPLOYMENT-NAME</code></p> </li> <li> <p>Set your Azure API key:    Use <code>--set-api-key</code> or add it to your config file:    <code>bash    janito --set-api-key YOUR_AZURE_OPENAI_KEY</code></p> </li> <li> <p>(Optional) Set API version:    If you need a specific API version, set <code>azure_openai_api_version</code> (default: <code>2023-05-15</code>).    <code>bash    janito --set-local-config azure_openai_api_version=2023-05-15</code></p> </li> </ol>"},{"location":"reference/azure-openai/#example-configuration","title":"Example Configuration","text":"<p>Here is an example of the relevant configuration keys:</p> <pre><code>api_key = \"YOUR_AZURE_OPENAI_KEY\"\nbase_url = \"https://YOUR-RESOURCE-NAME.openai.azure.com/openai/deployments/YOUR-DEPLOYMENT-NAME\"\nazure_openai_api_version = \"2023-05-15\"  # Optional\n</code></pre>"},{"location":"reference/azure-openai/#notes","title":"Notes","text":"<ul> <li>You can use either local or global config for these settings.</li> <li>For more information, see the main README and release notes.</li> </ul>"},{"location":"reference/cli-options/","title":"\ud83c\udfc1 Janito CLI Options","text":"<p>This page documents all command-line options for Janito, as shown by <code>janito --help</code>. These options temporarily override configuration for a single session and do not persist changes to config files.</p>"},{"location":"reference/cli-options/#overview","title":"\ud83d\udca1 Overview","text":"<p>These options are useful for one-off runs, scripting, or experimentation. They take precedence over config files for the current invocation only.</p>"},{"location":"reference/cli-options/#options","title":"\u2699\ufe0f Options","text":"Option Description <code>user_prompt</code> Prompt to submit (positional argument) <code>-h</code>, <code>--help</code> Show this help message and exit <code>--verbose-api</code> Print API calls and responses of LLM driver APIs for debugging/tracing. <code>--verbose-tools</code> Print info messages for tool execution in tools adapter. <code>--verbose-agent</code> Print info messages for agent event and message part handling. <code>-z</code>, <code>--zero</code> IDE zero mode: disables system prompt &amp; all tools for raw LLM interaction <code>-x</code>, <code>--exec</code> Enable execution/run tools (allows running code or shell tools from the CLI). Disabled by default for safety. <code>--unset KEY</code> Unset (remove) a config key <code>--version</code> Show program's version number and exit <code>--list-tools</code> List all registered tools <code>--show-config</code> Show the current config <code>--list-providers</code> List supported LLM providers <code>-l</code>, <code>--list-models</code> List all supported models <code>--set-api-key API_KEY</code> Set API key for the provider (requires -p PROVIDER) <code>--set [PROVIDER_NAME.]KEY=VALUE</code> Set a config key <code>-s SYSTEM_PROMPT</code>, <code>--system SYSTEM_PROMPT</code> Set a system prompt <code>-S</code>, <code>--show-system</code> Show the resolved system prompt for the main agent <code>-r ROLE</code>, <code>--role ROLE</code> Set the role for the agent <code>-p PROVIDER</code>, <code>--provider PROVIDER</code> Select the provider <code>-m MODEL</code>, <code>--model MODEL</code> Select the model <code>-t TEMPERATURE</code>, <code>--temperature TEMPERATURE</code> Set the temperature <code>-v</code>, <code>--verbose</code> Print extra information before answering <code>-R</code>, <code>--raw</code> Print the raw JSON response from the OpenAI API (if applicable) <code>-w</code>, <code>--web</code> Enable the builtin lightweight web file viewer for terminal links (disabled by default) <p>| <code>--termweb-port TERMWEB_PORT</code> | Port for the termweb server (default: 8088) | | <code>-e</code>, <code>--event-log</code> | Enable event logging to the system bus | | <code>--event-debug</code> | Print debug info on event subscribe/submit methods |</p>"},{"location":"reference/cli-options/#usage-example","title":"\ud83d\udc68\u200d\ud83d\udcbb Usage Example","text":"<pre><code>janito -p openai -m gpt-3.5-turbo \"Your prompt here\"\njanito --list-tools\njanito -w  # Enable clickable file links via web viewer (termweb)\n</code></pre>"},{"location":"reference/cli-options/#enabling-execution-tools","title":"\u26a0\ufe0f Enabling Execution Tools","text":"<p>By default, tools that can execute code or shell commands are disabled for safety. To enable these tools (such as code execution, shell commands, etc.), use the <code>--exec</code> or <code>-x</code> flag:</p> <pre><code>janito -x \"Run this code: print('Hello, world!')\"\n</code></pre> <p>Warning: Enabling execution tools allows running arbitrary code or shell commands. Only use <code>--exec</code> if you trust your prompt and environment.</p> <p>This page is generated from the output of <code>janito --help</code>.</p>"},{"location":"reference/message-handler-model/","title":"Message Handler Model","text":"<p>This document describes the message handler model used in Janito for both CLI and web output. For details on the styled terminal output, see the Rich Message Handler. The model ensures that all output\u2014whether from tools or from assistant/LLM content\u2014is routed through a single, consistent API, simplifying both backend and frontend logic.</p>"},{"location":"reference/message-handler-model/#overview","title":"Overview","text":"<ul> <li>Single handler for all output: tools, assistant, or system messages.</li> <li>Consistent message format: every message is sent with a message string and a message type.</li> <li>Easy to extend: add new message types or styles as needed.</li> </ul>"},{"location":"reference/message-handler-model/#message-format","title":"Message Format","text":"<p>A message is always represented as:</p> <ul> <li>message: The text to display (string)</li> <li>msg_type: The type/category of the message (string)</li> </ul> <p>For queue/web integration, each message is sent as a tuple:</p> <pre><code>('message', message, msg_type)\n</code></pre>"},{"location":"reference/message-handler-model/#common-msg_type-values","title":"Common <code>msg_type</code> Values","text":"<ul> <li><code>info</code>: Informational or neutral messages (default)</li> <li><code>success</code>: Successful operations (e.g., file created)</li> <li><code>error</code>: Errors or failures</li> <li><code>content</code>: Assistant/LLM responses or natural language content</li> <li>(You can add more types as needed)</li> </ul>"},{"location":"reference/message-handler-model/#handler-api","title":"Handler API","text":""},{"location":"reference/message-handler-model/#python-backend","title":"Python (Backend)","text":"<pre><code>handler.handle_message(msg, msg_type=None)\n</code></pre> <ul> <li><code>msg</code>: Either a string (content) or a dict with <code>{\"type\": ..., \"message\": ...}</code> (tool progress)</li> <li><code>msg_type</code>: Optional; used if <code>msg</code> is a string</li> </ul>"},{"location":"reference/message-handler-model/#example-usage","title":"Example Usage","text":"<pre><code># Tool output\nhandler.handle_message({\"type\": \"success\", \"message\": \"\u2705 File created\"})\n\n# Assistant/content output\nhandler.handle_message(\"Here is your summary...\", msg_type=\"content\")\n</code></pre>"},{"location":"reference/message-handler-model/#web-queue-integration","title":"Web Queue Integration","text":"<ul> <li>All output is sent to the frontend as:</li> <li><code>('message', message, msg_type)</code></li> <li>The frontend displays the message with styling based on <code>msg_type</code>.</li> </ul>"},{"location":"reference/message-handler-model/#frontend-handling","title":"Frontend Handling","text":"<ul> <li>Render all messages using a single handler/component.</li> <li>Style by <code>msg_type</code> (e.g., green for <code>success</code>, red for <code>error</code>, etc).</li> <li>No need to distinguish tool/content at the backend\u2014just use <code>msg_type</code>.</li> </ul>"},{"location":"reference/message-handler-model/#benefits","title":"Benefits","text":"<ul> <li>Consistent: Same styling and logic everywhere.</li> <li>Extensible: Add more message types or custom styles easily.</li> <li>Simple: Less boilerplate, easier to maintain.</li> </ul> <p>This model applies to both CLI and web output, making the Janito user experience clean, predictable, and easy to evolve.</p>"},{"location":"reference/rich-message-handler/","title":"Rich Message Handler","text":"<p>The Rich Message Handler is responsible for rendering all output (tool, agent, system) in the terminal using the rich library for styled and colorized output.</p>"},{"location":"reference/rich-message-handler/#features","title":"Features","text":"<ul> <li>Unified output: Handles all message types (tool, agent, system) through a single API.</li> <li>Styled messages: Uses colors and styles for different message types (info, success, error, warning, content, stdout, stderr).</li> <li>Markdown rendering: Renders assistant/content output as Markdown for improved readability.</li> <li>Trust mode: Suppresses all output except assistant/content if the <code>trust</code> config is enabled.</li> </ul>"},{"location":"reference/rich-message-handler/#supported-message-types","title":"Supported Message Types","text":"<ul> <li><code>content</code>: Rendered as Markdown (for assistant/LLM responses)</li> <li><code>info</code>: Cyan text</li> <li><code>success</code>: Bold green text</li> <li><code>error</code>: Bold red text</li> <li><code>warning</code>: Bold yellow text</li> <li><code>progress</code>: (Custom handler, e.g., progress bars)</li> <li><code>stdout</code>: Dark green background</li> <li><code>stderr</code>: Dark red background</li> </ul>"},{"location":"reference/rich-message-handler/#example-usage","title":"Example Usage","text":"<pre><code>handler = RichMessageHandler()\nhandler.handle_message({\"type\": \"success\", \"message\": \"\u2705 File created\"})\nhandler.handle_message({\"type\": \"content\", \"message\": \"**Hello!** This is Markdown.\"})\n</code></pre>"},{"location":"reference/rich-message-handler/#integration","title":"Integration","text":"<ul> <li>Used as the default message handler for CLI output in Janito.</li> <li>Honors the <code>trust</code> config to suppress non-content output for safer automation.</li> <li>Extensible: Add new message types or styles as needed.</li> </ul> <p>For the overall message handler model, see the Message Handler Model.</p>"}]}